{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ac9504",
   "metadata": {},
   "source": [
    "Nous allons dans notre projet avoir deux manières différentes de répondre à notre problèmatique qui est de détecter les grand feu qui sont les plus dangereux et dévastateur pour la planète afin d'aider les pompiers à connaître l'urgence de la situation en cas de départ de feu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9a0fd",
   "metadata": {},
   "source": [
    "Ici nous chargons notre jeu de donnée, nous avions initialement un jeu de donné qui était entre 1992 et 2020 mais nous avons réalisé qu'il y avait de nombreuses valeurs manquantes dans les premieres années du data set c'est pour cela que nous avons décidé de nous concentré sur l'année 2020. (d'autant plus que le data set complet mettais 20 minutes à se charger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45042094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axjui\\AppData\\Local\\Temp\\ipykernel_20032\\2025074942.py:2: DtypeWarning: Columns (14,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(\"2020_FPA_FOD_cons.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"2020_FPA_FOD_cons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b6de1",
   "metadata": {},
   "source": [
    "Le script cleaning.py est un pipeline complet de préparation de données spécialement conçu pour transformer des données brutes (probablement des relevés d'incendies et météo) en un format mathématique propre, digeste et optimisé pour un algorithme comme XGBoost.\n",
    "\n",
    "Voici l'analyse détaillée de la fonction finale full_data_cleaning_pipeline et comment elle orchestre les autres fonctions pour préparer le terrain au Machine Learning.\n",
    "\n",
    "La Fonction Maîtresse : full_data_cleaning_pipeline\n",
    "Cette fonction agit comme un chef d'orchestre. Elle prend le DataFrame brut en entrée et exécute 6 étapes séquentielles critiques. Si vous lancez cette fonction, voici ce qui arrive à vos données :\n",
    "\n",
    "Étape 1 : Le \"Grand Ménage\" (clean_dataframe)\n",
    "Action : Elle supprime les colonnes qui sont trop vides (plus de 50% de NaN) ou qui ne servent à rien (colonnes constantes où toutes les valeurs sont identiques)..\n",
    "\n",
    "Étape 2 : Correction des Erreurs de Capteurs (handle_aberrant_values)\n",
    "Action : Elle détecte des valeurs impossibles spécifiques aux capteurs météo/satellites (ex: -1e30, codes erreur -900, erreurs topo 32767). Elle remplace ces erreurs par NaN (valeur manquante).\n",
    "\n",
    "Étape 3 : Nettoyage Catégoriel et Temporel (clean_categorical_features)\n",
    "Action :\n",
    "\n",
    "Dates : Transforme DISCOVERY_DATE en chiffres utilisables : Mois et Jour de l'année (saisonnalité).\n",
    "\n",
    "Nettoyage : Supprime les identifiants uniques (FIRE_NAME, FPA_ID) qui causeraient du sur-apprentissage (overfitting) immédiat.\n",
    "\n",
    "Conversion : Force les colonnes qui ressemblent à du texte mais sont des chiffres (ex: NDVI) à devenir numériques.\n",
    "\n",
    "Gestion des NaN : Remplit les trous dans les catégories par le mot \"Missing\".\n",
    "\n",
    "Étape 4 : Imputation et Anti-Fuite (impute_nan_and_drop_manual_cols)\n",
    "Action :\n",
    "\n",
    "Anti-Fuite (Data Leakage) : Supprime les colonnes qui donnent la réponse à l'avance ou qui n'existent qu'après le feu (ex: CONT_DATE date de contrôle du feu). C'est crucial pour que le modèle soit réaliste.\n",
    "\n",
    "Imputation : Remplit les trous dans les colonnes numériques avec la Médiane.\n",
    "\n",
    "Temps : Convertit DISCOVERY_TIME en un chiffre décimal (ex: 14h30 devient 14.5).\n",
    "\n",
    "Étape 5 : Feature Engineering Avancé (add_advanced_features)\n",
    "\n",
    "Elle crée de nouvelles informations basées sur la physique du feu.\n",
    "\n",
    "Interactions : Elle multiplie le Vent par la Sécheresse (Wind_x_Dryness). Pour un feu, le vent n'est dangereux que si c'est sec.\n",
    "\n",
    "Clustering Spatial (KMeans) : Elle utilise la Latitude et la Longitude pour grouper les feux en 50 \"régions\" homogènes. Cela permet au modèle de comprendre \"Ah, on est dans la zone Sierra Nevada\" sans avoir besoin de cartes complexes.\n",
    "\n",
    "Étape 6 : Encodage Final (encode_final_categories)\n",
    "Action : Applique le One-Hot Encoding. Elle transforme les colonnes de texte restantes (ex: \"État\", \"Cause du feu\", \"Cluster Régional\") en colonnes binaires (0 ou 1).\n",
    "\n",
    "XGBoost ne lit que des chiffres. Il ne peut pas lire \"Californie\", mais il comprend très bien une colonne State_CA qui vaut 1.\n",
    "\n",
    "Résumé : Ce dataset est prêt pour XGBoost !\n",
    "À la sortie de full_data_cleaning_pipeline, votre DataFrame (df_final) possède les qualités suivantes :\n",
    "\n",
    "100% Numérique : Tout le texte a été encodé ou supprimé.\n",
    "\n",
    "Sans Fuite (Leakage Free) : Les variables du futur (post-incendie) sont retirées.\n",
    "\n",
    "Dense : Il n'y a plus de valeurs manquantes (imputées par médiane ou indicateur).\n",
    "\n",
    "Enrichi : Il contient des signaux forts (Saisonnalité, Interactions Vent/Sécheresse, Clusters géographiques) que XGBoost pourra exploiter pour maximiser sa performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5341fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de Nettoyage - Démarrage. Dimensions : (73221, 308)\n",
      "======================================================================\n",
      "Colonnes avec plus de 50.0% de NaN à supprimer : 23\n",
      "Colonnes constantes à supprimer : 8\n",
      "Total de colonnes supprimées (Step 1) : 24\n",
      "--- DÉBUT DU NETTOYAGE DÉTAILLÉ (handle_aberrant_values) ---\n",
      "   -> 225 colonnes float vérifiées.\n",
      "   -> 92 valeurs techniques (-1e30) remplacées par NaN.\n",
      "   -> 3409 codes d'erreur (< -900) remplacés par NaN.\n",
      "   -> 1474 erreurs topo (32767) remplacées par NaN.\n",
      "Total des cellules corrigées : 4975\n",
      "-> Features temporelles (Mois, Jour) extraites.\n",
      "-> Conversion des colonnes 'faux-textes' en numériques...\n",
      "-> Colonnes catégorielles supprimées (Manuelles) : 15\n",
      "-> Imputation des NaN catégoriels par 'Missing' sur 22 colonnes.\n",
      "Dimensions avant traitement des NaN : (73221, 271)\n",
      "   -> 'DISCOVERY_TIME' transformée en 'Discovery_Time_Hours'.\n",
      "-> Colonnes supprimées (Leakage/IDs) : 4\n",
      "-> Remplissage des valeurs manquantes (Médiane)...\n",
      "   ALERTE : 10 colonnes sont entièrement vides (ex: ['EVH_1km', 'EVT_1km', 'EVC_1km']).\n",
      "   -> Suppression de ces colonnes vides avant imputation pour éviter le crash.\n",
      "-> NaN restants dans les colonnes numériques : 0\n",
      "-> Création des features avancées (Physique & Clustering)...\n",
      "   -> Interactions physiques ajoutées (Wind_x_Dryness, etc.)\n",
      "   -> Clustering Spatial terminé (50 régions créées).\n",
      "-> Encodage One-Hot de 20 variables (ex: Cause, State, Ecoregion)...\n",
      "Dimensions APRÈS encodage : (73221, 696)\n",
      "======================================================================\n",
      "Pipeline terminé. Dimensions finales : (73221, 696)\n"
     ]
    }
   ],
   "source": [
    "from cleaning import *\n",
    "df_clean=full_data_cleaning_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb35c6",
   "metadata": {},
   "source": [
    "Le code suivant prépare et entraine le data set de manière suivante en utisant le XGboost comme modèle: \n",
    "1. Préparation des Données (prepare_data)\n",
    "Cette étape transforme les données brutes en un format utilisable par le modèle.\n",
    "\n",
    "Simplification des Classes (Target Engineering) : Le code convertit les classes de feux d'origine (de A à G) en 3 catégories simplifiées pour faciliter la prédiction :\n",
    "\n",
    "0 (Petit) : Classes A et B.\n",
    "\n",
    "1 (Moyen) : Classe C.\n",
    "\n",
    "2 (Grand/Danger) : Classes D, E, F, G.\n",
    "\n",
    "Nettoyage : Il nettoie les noms de colonnes (suppression des caractères spéciaux) et ne garde que les caractéristiques (SELECTED_FEATURES) jugées pertinentes (ex: météo, végétation, population, état).\n",
    "\n",
    "2. Stratégie d'Entraînement : Le \"Soft Undersampling\"\n",
    "Les grands feux sont rares (données déséquilibrées). Si on entraîne le modèle sur les données brutes, il ignorera les grands feux.\n",
    "\n",
    "Split : Séparation classique 80% entraînement / 20% test.\n",
    "\n",
    "Rééquilibrage (Ratio 4:2:1) : Le code force une distribution artificielle dans les données d'entraînement:\n",
    "\n",
    "Pour 1 Grand feu (Classe 2)...\n",
    "\n",
    "Il garde 2 Feux Moyens (Classe 1)...\n",
    "\n",
    "Et 4 Petits Feux (Classe 0).\n",
    "\n",
    "3. Modélisation (XGBoost)\n",
    "Le code utilise XGBoost Classifier\n",
    "\n",
    "Paramètres clés :\n",
    "\n",
    "objective='multi:softprob' : Le modèle ne donne pas juste une classe, mais la probabilité d'appartenir à chaque classe (ex: 10% Petit, 30% Moyen, 60% Grand).\n",
    "\n",
    "n_estimators=500 & learning_rate=0.03 : Une configuration robuste qui crée beaucoup d'arbres (500) mais apprend lentement (0.03) pour éviter le sur-apprentissage (overfitting).\n",
    "\n",
    "4. Évaluation et Analyse de Risque\n",
    "Une fois le modèle entraîné, le code ne se contente pas de calculer la précision globale (qui serait trompeuse), il se concentre sur la détection du risque.\n",
    "\n",
    "A. Score de Risque\n",
    "Il isole la probabilité que le feu soit de Classe 2 (Grand Feu). C'est ce score (Score_Risque_GrandFeu) qui servira d'alerte.\n",
    "\n",
    "B. Métriques Clés Affichées\n",
    "AUC (Area Under Curve) : Une note globale sur 1.000. Elle mesure la capacité du modèle à classer un grand feu \"plus haut\" qu'un petit feu.\n",
    "\n",
    "Fiabilité (Calibration) : Le tableau \"Tranche de Risque\" vérifie la cohérence.\n",
    "\n",
    "Idéalement : Si le modèle prédit un risque de 80-100%, on veut voir que dans la réalité, ~80-100% de ces feux étaient effectivement des grands feux.\n",
    "\n",
    "Seuil de 50% (Analyse Décisionnelle) :\n",
    "\n",
    "Le code regarde ce qu'il se passe si on déclenche une alerte \"DANGER\" dès que la probabilité dépasse 50%.\n",
    "\n",
    "Il calcule combien de Grands Feux sont correctement détectés (Recall) et combien de Petits Feux déclenchent une fausse alerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48349c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PRÉPARATION FINALE AVANT MODÉLISATION ---\n",
      "-> Données prêtes. Dimensions : (73221, 51)\n",
      "\n",
      "--- ENTRAÎNEMENT DU MODÈLE XGBOOST ---\n",
      "-> Modèle entraîné avec succès.\n",
      "\n",
      "================================================================================\n",
      " CALCUL DES RÉSULTATS DÉTAILLÉS\n",
      "================================================================================\n",
      "SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : 0.902\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANALYSE DE RISQUE (Seuil > 50%)\n",
      "Petits Feux  détectés à haut risque :   1.34% (175/13062)\n",
      "Moyens Feux  détectés à haut risque :   6.76% (79/1169)\n",
      "Grands Feux  détectés à haut risque :  32.85% (136/414)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION (Les 50 Variables \"Élues\")\n",
    "# ==============================================================================\n",
    "SELECTED_FEATURES = [\n",
    "    'LONGITUDE', 'EVH', 'EVC', 'Population', 'Popo_1km', 'GDP', 'EBLR_PFS', \n",
    "    'No_FireStation_5.0km', 'GHM', 'Ecoregion_US_L3CODE', 'Ecoregion_NA_L2CODE', \n",
    "    'Ecoregion_NA_L1CODE', 'Wind_x_Dryness', 'NWCG_REPORTING_AGENCY_FWS', \n",
    "    'NWCG_CAUSE_CLASSIFICATION_Natural', 'NWCG_GENERAL_CAUSE_Natural', \n",
    "    'NWCG_GENERAL_CAUSE_Recreation and ceremony', 'OWNER_DESCR_MUNICIPAL/LOCAL', \n",
    "    'OWNER_DESCR_PRIVATE', 'OWNER_DESCR_UNDEFINED FEDERAL', 'STATE_AL', \n",
    "    'STATE_AZ', 'STATE_KS', 'STATE_KY', 'STATE_NC', 'STATE_ND', 'STATE_OK', \n",
    "    'STATE_SC', 'STATE_UT', 'STATE_WA', 'Mang_Type_TRIB', 'Mang_Type_UNK', \n",
    "    'Des_Tp_NF', 'NAME_Blue Mountains', 'NAME_Dissected Appalachian Plateau', \n",
    "    'NAME_Flint Hills', 'NAME_High Lava Plateau and Semiarid Hills', \n",
    "    'NAME_Middle Rockies (Townsend-Elkhorn)', 'NAME_Northern Cross Timbers and Lower Canadian Hills', \n",
    "    'NAME_Snake River Plain', 'NAME_South Central California Foothills and Coastal Mountains', \n",
    "    'NAME_Western Corn-Belt and Central Irregular Plains', 'fm100_Percentile_70-90%', \n",
    "    'vpd_Percentile_5-10%', 'Region_Cluster_2', 'Region_Cluster_3', \n",
    "    'Region_Cluster_12', 'Region_Cluster_28', 'Region_Cluster_41', 'Region_Cluster_44'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PRÉPARATION DES DONNÉES\n",
    "# ==============================================================================\n",
    "print(\"--- PRÉPARATION FINALE AVANT MODÉLISATION ---\")\n",
    "\n",
    "# Copie pour ne pas écraser l'original\n",
    "df_model = df_clean.copy()\n",
    "\n",
    "# 1. Mapping Target\n",
    "mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "if 'FIRE_SIZE_CLASS' in df_model.columns:\n",
    "    if df_model['FIRE_SIZE_CLASS'].dtype == 'object':\n",
    "        df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(str).map(mapping)\n",
    "    df_model = df_model.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "    df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "# 2. Nettoyage noms colonnes (Regex XGBoost)\n",
    "new_cols = {c: re.sub(r'[\\[\\]<]', '_', c).strip() for c in df_model.columns}\n",
    "df_model = df_model.rename(columns=new_cols)\n",
    "\n",
    "# 3. Filtrage\n",
    "cols_to_keep = SELECTED_FEATURES + ['FIRE_SIZE_CLASS']\n",
    "target_series = df_model['FIRE_SIZE_CLASS']\n",
    "df_optimized = df_model.reindex(columns=cols_to_keep, fill_value=0)\n",
    "df_optimized['FIRE_SIZE_CLASS'] = target_series\n",
    "\n",
    "print(f\"-> Données prêtes. Dimensions : {df_optimized.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SPLIT & ENTRAÎNEMENT (Soft Undersampling)\n",
    "# ==============================================================================\n",
    "X = df_optimized.drop(columns=['FIRE_SIZE_CLASS'])\n",
    "y = df_optimized['FIRE_SIZE_CLASS']\n",
    "\n",
    "print(\"\\n--- ENTRAÎNEMENT DU MODÈLE XGBOOST ---\")\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Équilibrage 4:2:1\n",
    "train_data = pd.concat([X_train_raw, y_train_raw], axis=1)\n",
    "df_0 = train_data[train_data.FIRE_SIZE_CLASS == 0]\n",
    "df_1 = train_data[train_data.FIRE_SIZE_CLASS == 1]\n",
    "df_2 = train_data[train_data.FIRE_SIZE_CLASS == 2]\n",
    "\n",
    "n_minority = len(df_2)\n",
    "df_0_down = resample(df_0, replace=False, n_samples=min(len(df_0), n_minority * 4), random_state=42)\n",
    "df_1_down = resample(df_1, replace=False, n_samples=min(len(df_1), n_minority * 2), random_state=42)\n",
    "df_train_balanced = pd.concat([df_0_down, df_1_down, df_2]).sample(frac=1, random_state=42)\n",
    "\n",
    "X_train = df_train_balanced.drop(columns=['FIRE_SIZE_CLASS'])\n",
    "y_train = df_train_balanced['FIRE_SIZE_CLASS']\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob', \n",
    "    num_class=3,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"-> Modèle entraîné avec succès.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PRÉDICTIONS ET STOCKAGE COMPLET\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CALCUL DES RÉSULTATS DÉTAILLÉS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "results = X_test.copy()\n",
    "results['True_Class'] = y_test            \n",
    "results['Prob_Class_0'] = probas[:, 0]    \n",
    "results['Prob_Class_1'] = probas[:, 1]    \n",
    "results['Prob_Class_2'] = probas[:, 2]    \n",
    "\n",
    "# Calculs additionnels\n",
    "results['Score_Risque_GrandFeu'] = results['Prob_Class_2']\n",
    "results['Score_Risque_Percent'] = (results['Score_Risque_GrandFeu'] * 100).round(1)\n",
    "\n",
    "# A. Score Global (AUC)\n",
    "y_test_binary = (y_test == 2).astype(int) \n",
    "auc = roc_auc_score(y_test_binary, results['Prob_Class_2'])\n",
    "print(f\"SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : {auc:.3f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# B. Analyse par seuil de 50%\n",
    "print(\"\\nANALYSE DE RISQUE (Seuil > 50%)\")\n",
    "for cl, label in [(0, \"Petits Feux\"), (1, \"Moyens Feux\"), (2, \"Grands Feux\")]:\n",
    "    # Correction ici : utilisation de 'True_Class' au lieu de 'Vrai_Classe'\n",
    "    subset = results[results['True_Class'] == cl]\n",
    "    nb_alert = (subset['Score_Risque_GrandFeu'] > 0.5).sum()\n",
    "    pct_alert = (nb_alert / len(subset)) * 100 if len(subset) > 0 else 0\n",
    "    print(f\"{label:<12} détectés à haut risque : {pct_alert:6.2f}% ({nb_alert}/{len(subset)})\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a2232",
   "metadata": {},
   "source": [
    "Ici on à une très bonne compréhension du data set avec une AUC de 0.902 c'est exceptionnel cependant malgrès cela, nos résultats ne sont pas tant utile à nos pompiers car notre modèle ne detecte que très peu les grands feu, il faut donc changer quelque chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6064f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> ANALYSE DE FIABILITÉ : PETIT FEU (Classe 0)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Petit Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 1001       | 49.7%\n",
      "20-40%               | 1706       | 72.1%\n",
      "40-60%               | 1887       | 84.7%\n",
      "60-80%               | 2791       | 93.1%\n",
      "80-100%              | 7260       | 98.3%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : MOYEN FEU (Classe 1)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Moyen Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 8625       | 2.1%\n",
      "20-40%               | 3136       | 9.9%\n",
      "40-60%               | 1996       | 20.5%\n",
      "60-80%               | 862        | 30.3%\n",
      "80-100%              | 26         | 38.5%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : GRAND FEU (Classe 2)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Grand Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 12794      | 1.0%\n",
      "20-40%               | 1168       | 8.6%\n",
      "40-60%               | 501        | 23.2%\n",
      "60-80%               | 159        | 37.1%\n",
      "80-100%              | 23         | 60.9%\n",
      "-----------------------------------------------------------------\n",
      "Rappels par classe (0, 1, 2) : [0.84627163 0.51668092 0.43236715]\n",
      "RECALL DES GRANDS FEUX (Classe 2) : 43.24%\n"
     ]
    }
   ],
   "source": [
    "def print_calibration_table_notebook(df_results, class_id, class_name):\n",
    "    \"\"\"\n",
    "    Affiche la table de fiabilité pour une classe donnée avec le formatage spécifique demandé.\n",
    "    \"\"\"\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    \n",
    "    # 1. Création des tranches de probabilités (bins)\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    \n",
    "    # On travaille sur une copie pour ne pas modifier l'original\n",
    "    df_temp = df_results.copy()\n",
    "    df_temp['Tranche'] = pd.cut(df_temp[col_score], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # 2. Affichage de l'en-tête\n",
    "    print(f\"\\n>>> ANALYSE DE FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    # 3. Calculs et affichage ligne par ligne\n",
    "    # observed=False permet d'afficher même les tranches vides (si catégories pandas)\n",
    "    grouped = df_temp.groupby('Tranche', observed=False)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in grouped.groups:\n",
    "            group = grouped.get_group(label)\n",
    "            count = len(group)\n",
    "            if count > 0:\n",
    "                # Calcul du pourcentage de Vrais Positifs dans cette tranche\n",
    "                real_percentage = (group['True_Class'] == class_id).mean() * 100\n",
    "                print(f\"{label:<20} | {count:<10} | {real_percentage:.1f}%\")\n",
    "            else:\n",
    "                 print(f\"{label:<20} | 0          | -\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "            \n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# --- EXÉCUTION DE L'ANALYSE ---\n",
    "\n",
    "# S'assurer que 'results' est bien défini (issu du bloc précédent)\n",
    "if 'results' in locals():\n",
    "    print_calibration_table_notebook(results, 0, \"Petit Feu\")\n",
    "    print_calibration_table_notebook(results, 1, \"Moyen Feu\")\n",
    "    print_calibration_table_notebook(results, 2, \"Grand Feu\")\n",
    "else:\n",
    "    print(\"Erreur : Le DataFrame 'results' est introuvable. Veuillez exécuter la cellule d'entraînement précédente.\")\n",
    "from sklearn.metrics import recall_score\n",
    "y_pred = model.predict(X_test)\n",
    "recall_scores = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "recall_grand_feu = recall_scores[2]\n",
    "\n",
    "print(f\"Rappels par classe (0, 1, 2) : {recall_scores}\")\n",
    "print(f\"RECALL DES GRANDS FEUX (Classe 2) : {recall_grand_feu:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aa823",
   "metadata": {},
   "source": [
    "Les resultats ici confirme notre première analyse, on à un recall des grands feux de seulement 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac513b0",
   "metadata": {},
   "source": [
    "Nous allons essayer d'utiliser un modèle beaucoup plus agressif en utilisant des poids de réequilibrage plus poussé : [1, 5, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab34ae6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PRÉPARATION FINALE AVANT MODÉLISATION (MODE AGRESSIF) ---\n",
      "-> Données prêtes. Dimensions : (73221, 51)\n",
      "\n",
      "--- ENTRAÎNEMENT DU MODÈLE XGBOOST (COST-SENSITIVE) ---\n",
      "-> Poids appliqués : {0: 1, 1: 5, 2: 20}\n",
      "-> Exemple de poids (10 premiers) : [ 1  5 20  1  1  1  1  1  1  1]\n",
      "-> Modèle entraîné avec pondération agressive.\n",
      "\n",
      "================================================================================\n",
      " CALCUL DES RÉSULTATS DÉTAILLÉS (APPROCHE AGRESSIVE)\n",
      "================================================================================\n",
      "SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : 0.901\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANALYSE DE RISQUE (Seuil > 50%)\n",
      "Petits Feux  détectés à haut risque :   3.47% (453/13062)\n",
      "Moyens Feux  détectés à haut risque :  17.11% (200/1169)\n",
      "Grands Feux  détectés à haut risque :  51.21% (212/414)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION (Les 50 Variables \"Élues\")\n",
    "# ==============================================================================\n",
    "SELECTED_FEATURES = [\n",
    "    'LONGITUDE', 'EVH', 'EVC', 'Population', 'Popo_1km', 'GDP', 'EBLR_PFS', \n",
    "    'No_FireStation_5.0km', 'GHM', 'Ecoregion_US_L3CODE', 'Ecoregion_NA_L2CODE', \n",
    "    'Ecoregion_NA_L1CODE', 'Wind_x_Dryness', 'NWCG_REPORTING_AGENCY_FWS', \n",
    "    'NWCG_CAUSE_CLASSIFICATION_Natural', 'NWCG_GENERAL_CAUSE_Natural', \n",
    "    'NWCG_GENERAL_CAUSE_Recreation and ceremony', 'OWNER_DESCR_MUNICIPAL/LOCAL', \n",
    "    'OWNER_DESCR_PRIVATE', 'OWNER_DESCR_UNDEFINED FEDERAL', 'STATE_AL', \n",
    "    'STATE_AZ', 'STATE_KS', 'STATE_KY', 'STATE_NC', 'STATE_ND', 'STATE_OK', \n",
    "    'STATE_SC', 'STATE_UT', 'STATE_WA', 'Mang_Type_TRIB', 'Mang_Type_UNK', \n",
    "    'Des_Tp_NF', 'NAME_Blue Mountains', 'NAME_Dissected Appalachian Plateau', \n",
    "    'NAME_Flint Hills', 'NAME_High Lava Plateau and Semiarid Hills', \n",
    "    'NAME_Middle Rockies (Townsend-Elkhorn)', 'NAME_Northern Cross Timbers and Lower Canadian Hills', \n",
    "    'NAME_Snake River Plain', 'NAME_South Central California Foothills and Coastal Mountains', \n",
    "    'NAME_Western Corn-Belt and Central Irregular Plains', 'fm100_Percentile_70-90%', \n",
    "    'vpd_Percentile_5-10%', 'Region_Cluster_2', 'Region_Cluster_3', \n",
    "    'Region_Cluster_12', 'Region_Cluster_28', 'Region_Cluster_41', 'Region_Cluster_44'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PRÉPARATION DES DONNÉES\n",
    "# ==============================================================================\n",
    "print(\"--- PRÉPARATION FINALE AVANT MODÉLISATION (MODE AGRESSIF) ---\")\n",
    "\n",
    "# Copie de sécurité\n",
    "df_model = df_clean.copy()\n",
    "\n",
    "# 1. Mapping Target\n",
    "mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "if 'FIRE_SIZE_CLASS' in df_model.columns:\n",
    "    if df_model['FIRE_SIZE_CLASS'].dtype == 'object':\n",
    "        df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(str).map(mapping)\n",
    "    df_model = df_model.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "    df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "# 2. Nettoyage noms colonnes (Regex XGBoost)\n",
    "new_cols = {c: re.sub(r'[\\[\\]<]', '_', c).strip() for c in df_model.columns}\n",
    "df_model = df_model.rename(columns=new_cols)\n",
    "\n",
    "# 3. Filtrage\n",
    "cols_to_keep = SELECTED_FEATURES + ['FIRE_SIZE_CLASS']\n",
    "target_series = df_model['FIRE_SIZE_CLASS']\n",
    "df_optimized = df_model.reindex(columns=cols_to_keep, fill_value=0)\n",
    "df_optimized['FIRE_SIZE_CLASS'] = target_series\n",
    "\n",
    "print(f\"-> Données prêtes. Dimensions : {df_optimized.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SPLIT & ENTRAÎNEMENT (Pondération 1:5:20)\n",
    "# ==============================================================================\n",
    "X = df_optimized.drop(columns=['FIRE_SIZE_CLASS'])\n",
    "y = df_optimized['FIRE_SIZE_CLASS']\n",
    "\n",
    "print(\"\\n--- ENTRAÎNEMENT DU MODÈLE XGBOOST (COST-SENSITIVE) ---\")\n",
    "\n",
    "# Split classique (sans resampling, car on utilise des poids)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# CRÉATION DES POIDS [1, 5, 20]\n",
    "# 0 (Petit) = Poids 1\n",
    "# 1 (Moyen) = Poids 5\n",
    "# 2 (Grand) = Poids 20\n",
    "weights_map = {0: 1, 1: 5, 2: 20}\n",
    "sample_weights = y_train.map(weights_map)\n",
    "\n",
    "print(f\"-> Poids appliqués : {weights_map}\")\n",
    "print(f\"-> Exemple de poids (10 premiers) : {sample_weights.head(10).values}\")\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob', \n",
    "    num_class=3,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entraînement avec sample_weight\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "print(\"-> Modèle entraîné avec pondération agressive.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PRÉDICTIONS ET STOCKAGE COMPLET\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CALCUL DES RÉSULTATS DÉTAILLÉS (APPROCHE AGRESSIVE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "results = X_test.copy()\n",
    "results['True_Class'] = y_test            \n",
    "results['Prob_Class_0'] = probas[:, 0]    \n",
    "results['Prob_Class_1'] = probas[:, 1]    \n",
    "results['Prob_Class_2'] = probas[:, 2]    \n",
    "\n",
    "# Calculs additionnels\n",
    "results['Score_Risque_GrandFeu'] = results['Prob_Class_2']\n",
    "results['Score_Risque_Percent'] = (results['Score_Risque_GrandFeu'] * 100).round(1)\n",
    "\n",
    "# A. Score Global (AUC)\n",
    "y_test_binary = (y_test == 2).astype(int) \n",
    "auc = roc_auc_score(y_test_binary, results['Prob_Class_2'])\n",
    "print(f\"SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : {auc:.3f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# B. Analyse par seuil de 50%\n",
    "print(\"\\nANALYSE DE RISQUE (Seuil > 50%)\")\n",
    "for cl, label in [(0, \"Petits Feux\"), (1, \"Moyens Feux\"), (2, \"Grands Feux\")]:\n",
    "    subset = results[results['True_Class'] == cl]\n",
    "    nb_alert = (subset['Score_Risque_GrandFeu'] > 0.5).sum()\n",
    "    pct_alert = (nb_alert / len(subset)) * 100 if len(subset) > 0 else 0\n",
    "    print(f\"{label:<12} détectés à haut risque : {pct_alert:6.2f}% ({nb_alert}/{len(subset)})\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. FONCTION D'ANALYSE DE CALIBRATION\n",
    "# ==============================================================================\n",
    "def print_calibration_table_notebook(df_results, class_id, class_name):\n",
    "    \"\"\"\n",
    "    Affiche la table de fiabilité pour une classe donnée.\n",
    "    \"\"\"\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    \n",
    "    # 1. Création des tranches de probabilités (bins)\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    \n",
    "    # Copie locale\n",
    "    df_temp = df_results.copy()\n",
    "    df_temp['Tranche'] = pd.cut(df_temp[col_score], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # 2. Affichage de l'en-tête\n",
    "    print(f\"\\n>>> ANALYSE DE FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    # 3. Calculs et affichage ligne par ligne\n",
    "    grouped = df_temp.groupby('Tranche', observed=False)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in grouped.groups:\n",
    "            group = grouped.get_group(label)\n",
    "            count = len(group)\n",
    "            if count > 0:\n",
    "                # Calcul du pourcentage de Vrais Positifs dans cette tranche\n",
    "                real_percentage = (group['True_Class'] == class_id).mean() * 100\n",
    "                print(f\"{label:<20} | {count:<10} | {real_percentage:.1f}%\")\n",
    "            else:\n",
    "                 print(f\"{label:<20} | 0          | -\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "            \n",
    "    print(\"-\" * 65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9931dec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> ANALYSE DÉTAILLÉE : PETIT FEU (Classe 0)\n",
      "Tranche    | Nb Feux  || % Vrai 0  | % Vrai 1  | % Vrai 2 \n",
      "(Proba)    | (Total)  || (Petit)   | (Moyen)   | (Grand)  \n",
      "-----------------------------------------------------------------\n",
      "0-20%      | 1025     ||    48.5%  |    30.4%  |    21.1%\n",
      "20-40%     | 1629     ||    71.6%  |    23.0%  |     5.4%\n",
      "40-60%     | 1918     ||    85.4%  |    10.8%  |     3.8%\n",
      "60-80%     | 2663     ||    93.0%  |     6.1%  |     0.9%\n",
      "80-100%    | 7410     ||    98.3%  |     1.5%  |     0.2%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DÉTAILLÉE : MOYEN FEU (Classe 1)\n",
      "Tranche    | Nb Feux  || % Vrai 0  | % Vrai 1  | % Vrai 2 \n",
      "(Proba)    | (Total)  || (Petit)   | (Moyen)   | (Grand)  \n",
      "-----------------------------------------------------------------\n",
      "0-20%      | 10073    ||    94.9%  |     3.1%  |     2.0%\n",
      "20-40%     | 2753     ||    80.6%  |    14.1%  |     5.3%\n",
      "40-60%     | 1538     ||    72.6%  |    24.1%  |     3.4%\n",
      "60-80%     | 281      ||    61.6%  |    34.5%  |     3.9%\n",
      "80-100%    | 0        ||    -      |    -      |    -     \n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DÉTAILLÉE : GRAND FEU (Classe 2)\n",
      "Tranche    | Nb Feux  || % Vrai 0  | % Vrai 1  | % Vrai 2 \n",
      "(Proba)    | (Total)  || (Petit)   | (Moyen)   | (Grand)  \n",
      "-----------------------------------------------------------------\n",
      "0-20%      | 11584    ||    94.0%  |     5.4%  |     0.6%\n",
      "20-40%     | 1728     ||    80.1%  |    14.4%  |     5.5%\n",
      "40-60%     | 797      ||    68.0%  |    21.2%  |    10.8%\n",
      "60-80%     | 436      ||    48.4%  |    23.2%  |    28.4%\n",
      "80-100%    | 100      ||    37.0%  |    19.0%  |    44.0%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "RAPPEL (RECALL) GLOBAL :\n",
      "Petit : 86.08% | Moyen : 36.44% | Grand : 59.90%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# ==============================================================================\n",
    "# FONCTION D'ANALYSE DÉTAILLÉE (AVEC RÉPARTITION RÉELLE)\n",
    "# ==============================================================================\n",
    "def afficher_analyse_fiabilite_detaillee(df, class_id, nom_classe):\n",
    "    \"\"\"\n",
    "    Analyse la fiabilité par tranche de probabilité et montre \n",
    "    la répartition réelle des classes (0, 1, 2) dans chaque tranche.\n",
    "    \"\"\"\n",
    "    col_proba = f'Prob_Class_{class_id}'\n",
    "    \n",
    "    # Bins de probabilité\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    \n",
    "    # Copie de travail\n",
    "    df_work = df.copy()\n",
    "    df_work['Tranche'] = pd.cut(df_work[col_proba], bins=bins, labels=labels, include_lowest=True)\n",
    "    \n",
    "    # En-tête du tableau\n",
    "    print(f\"\\n>>> ANALYSE DÉTAILLÉE : {nom_classe.upper()} (Classe {class_id})\")\n",
    "    # On prépare un header plus large pour voir la répartition\n",
    "    print(f\"{'Tranche':<10} | {'Nb Feux':<8} || {'% Vrai 0':<9} | {'% Vrai 1':<9} | {'% Vrai 2':<9}\")\n",
    "    print(f\"{'(Proba)':<10} | {'(Total)':<8} || {'(Petit)':<9} | {'(Moyen)':<9} | {'(Grand)':<9}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Boucle de filtrage direct (Robuste)\n",
    "    for label in labels:\n",
    "        subset = df_work[df_work['Tranche'] == label]\n",
    "        count = len(subset)\n",
    "        \n",
    "        if count > 0:\n",
    "            # Calcul de la répartition réelle dans cette tranche\n",
    "            pct_0 = (subset['True_Class'] == 0).mean() * 100\n",
    "            pct_1 = (subset['True_Class'] == 1).mean() * 100\n",
    "            pct_2 = (subset['True_Class'] == 2).mean() * 100\n",
    "            \n",
    "            # Mise en évidence de la classe cible avec une *\n",
    "            # (optionnel, mais aide à la lecture)\n",
    "            print(f\"{label:<10} | {count:<8} || {pct_0:7.1f}%  | {pct_1:7.1f}%  | {pct_2:7.1f}%\")\n",
    "        else:\n",
    "            print(f\"{label:<10} | 0        ||    -      |    -      |    -     \")\n",
    "            \n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXÉCUTION\n",
    "# ==============================================================================\n",
    "\n",
    "# On lance l'analyse détaillée\n",
    "# Cela répond à votre question : \"Quand le modèle est sûr à 80% que c'est un Grand Feu,\n",
    "# est-ce que les erreurs sont des Moyens ou des Petits ?\"\n",
    "\n",
    "afficher_analyse_fiabilite_detaillee(results, 0, \"Petit Feu\")\n",
    "afficher_analyse_fiabilite_detaillee(results, 1, \"Moyen Feu\")\n",
    "afficher_analyse_fiabilite_detaillee(results, 2, \"Grand Feu\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RAPPEL DU RECALL GLOBAL\n",
    "# ==============================================================================\n",
    "y_true = results['True_Class']\n",
    "y_pred = results[['Prob_Class_0', 'Prob_Class_1', 'Prob_Class_2']].idxmax(axis=1).str.extract(r'(\\d+)').astype(int)[0]\n",
    "recalls = recall_score(y_true, y_pred, average=None)\n",
    "\n",
    "print(f\"\\nRAPPEL (RECALL) GLOBAL :\")\n",
    "print(f\"Petit : {recalls[0]:.2%} | Moyen : {recalls[1]:.2%} | Grand : {recalls[2]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b845dee",
   "metadata": {},
   "source": [
    "Nous avons ici un bien meilleur résultat, qui nous permet d'avoir un bon pourcentage de recall mais avec cependant peut être un peu trop d'alerte sur des feu, nous allons maintenant essaye un modèle très agressif xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b030885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PRÉPARATION FINALE AVANT MODÉLISATION (MODE OPTIMISÉ) ---\n",
      "-> Données prêtes. Dimensions : (73221, 51)\n",
      "\n",
      "--- ENTRAÎNEMENT DU MODÈLE XGBOOST (PARAMS OPTIMISÉS) ---\n",
      "-> Poids appliqués : {0: 1, 1: 10, 2: 50}\n",
      "-> Modèle entraîné avec succès (Hyperparamètres chargés).\n",
      "\n",
      "================================================================================\n",
      " CALCUL DES RÉSULTATS DÉTAILLÉS (APPROCHE 1:10:50)\n",
      "================================================================================\n",
      "SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : 0.893\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ANALYSE DE RISQUE (Seuil > 50%)\n",
      "Petits Feux  détectés à haut risque :   5.69% (743/13062)\n",
      "Moyens Feux  détectés à haut risque :  21.64% (253/1169)\n",
      "Grands Feux  détectés à haut risque :  60.39% (250/414)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : GRANDS FEUX (Classe 2)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Grands Feux\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 11007      | 0.6%\n",
      "20-40%               | 1822       | 3.5%\n",
      "40-60%               | 983        | 9.4%\n",
      "60-80%               | 597        | 18.4%\n",
      "80-100%              | 236        | 36.9%\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. FONCTION D'ANALYSE (Définie au début pour être disponible)\n",
    "# ==============================================================================\n",
    "def print_calibration_table_notebook(df_results, class_id, class_name):\n",
    "    \"\"\"\n",
    "    Affiche la table de fiabilité pour une classe donnée.\n",
    "    \"\"\"\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    \n",
    "    # Création des tranches de probabilités (bins)\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    \n",
    "    # Copie locale\n",
    "    df_temp = df_results.copy()\n",
    "    df_temp['Tranche'] = pd.cut(df_temp[col_score], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    print(f\"\\n>>> ANALYSE DE FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    grouped = df_temp.groupby('Tranche', observed=False)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in grouped.groups:\n",
    "            group = grouped.get_group(label)\n",
    "            count = len(group)\n",
    "            if count > 0:\n",
    "                real_percentage = (group['True_Class'] == class_id).mean() * 100\n",
    "                print(f\"{label:<20} | {count:<10} | {real_percentage:.1f}%\")\n",
    "            else:\n",
    "                 print(f\"{label:<20} | 0          | -\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION (Les 50 Variables \"Élues\")\n",
    "# ==============================================================================\n",
    "SELECTED_FEATURES = [\n",
    "    'LONGITUDE', 'EVH', 'EVC', 'Population', 'Popo_1km', 'GDP', 'EBLR_PFS', \n",
    "    'No_FireStation_5.0km', 'GHM', 'Ecoregion_US_L3CODE', 'Ecoregion_NA_L2CODE', \n",
    "    'Ecoregion_NA_L1CODE', 'Wind_x_Dryness', 'NWCG_REPORTING_AGENCY_FWS', \n",
    "    'NWCG_CAUSE_CLASSIFICATION_Natural', 'NWCG_GENERAL_CAUSE_Natural', \n",
    "    'NWCG_GENERAL_CAUSE_Recreation and ceremony', 'OWNER_DESCR_MUNICIPAL/LOCAL', \n",
    "    'OWNER_DESCR_PRIVATE', 'OWNER_DESCR_UNDEFINED FEDERAL', 'STATE_AL', \n",
    "    'STATE_AZ', 'STATE_KS', 'STATE_KY', 'STATE_NC', 'STATE_ND', 'STATE_OK', \n",
    "    'STATE_SC', 'STATE_UT', 'STATE_WA', 'Mang_Type_TRIB', 'Mang_Type_UNK', \n",
    "    'Des_Tp_NF', 'NAME_Blue Mountains', 'NAME_Dissected Appalachian Plateau', \n",
    "    'NAME_Flint Hills', 'NAME_High Lava Plateau and Semiarid Hills', \n",
    "    'NAME_Middle Rockies (Townsend-Elkhorn)', 'NAME_Northern Cross Timbers and Lower Canadian Hills', \n",
    "    'NAME_Snake River Plain', 'NAME_South Central California Foothills and Coastal Mountains', \n",
    "    'NAME_Western Corn-Belt and Central Irregular Plains', 'fm100_Percentile_70-90%', \n",
    "    'vpd_Percentile_5-10%', 'Region_Cluster_2', 'Region_Cluster_3', \n",
    "    'Region_Cluster_12', 'Region_Cluster_28', 'Region_Cluster_41', 'Region_Cluster_44'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PRÉPARATION DES DONNÉES\n",
    "# ==============================================================================\n",
    "print(\"--- PRÉPARATION FINALE AVANT MODÉLISATION (MODE OPTIMISÉ) ---\")\n",
    "\n",
    "# Copie de sécurité (Assure-toi que df_clean est chargé en mémoire)\n",
    "df_model = df_clean.copy()\n",
    "\n",
    "# 1. Mapping Target\n",
    "mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "if 'FIRE_SIZE_CLASS' in df_model.columns:\n",
    "    if df_model['FIRE_SIZE_CLASS'].dtype == 'object':\n",
    "        df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(str).map(mapping)\n",
    "    df_model = df_model.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "    df_model['FIRE_SIZE_CLASS'] = df_model['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "# 2. Nettoyage noms colonnes (Regex XGBoost)\n",
    "new_cols = {c: re.sub(r'[\\[\\]<]', '_', c).strip() for c in df_model.columns}\n",
    "df_model = df_model.rename(columns=new_cols)\n",
    "\n",
    "# 3. Filtrage\n",
    "cols_to_keep = SELECTED_FEATURES + ['FIRE_SIZE_CLASS']\n",
    "target_series = df_model['FIRE_SIZE_CLASS']\n",
    "df_optimized = df_model.reindex(columns=cols_to_keep, fill_value=0)\n",
    "df_optimized['FIRE_SIZE_CLASS'] = target_series\n",
    "\n",
    "print(f\"-> Données prêtes. Dimensions : {df_optimized.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SPLIT & ENTRAÎNEMENT (Pondération 1:10:50 & Hyperparamètres)\n",
    "# ==============================================================================\n",
    "X = df_optimized.drop(columns=['FIRE_SIZE_CLASS'])\n",
    "y = df_optimized['FIRE_SIZE_CLASS']\n",
    "\n",
    "print(\"\\n--- ENTRAÎNEMENT DU MODÈLE XGBOOST (PARAMS OPTIMISÉS) ---\")\n",
    "\n",
    "# Split classique\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# A. CRÉATION DES POIDS (Nouveaux poids : 1, 10, 50)\n",
    "weights_map = {0: 1, 1: 10, 2: 50}\n",
    "sample_weights = y_train.map(weights_map)\n",
    "\n",
    "print(f\"-> Poids appliqués : {weights_map}\")\n",
    "\n",
    "# B. DÉFINITION DES PARAMÈTRES OPTIMISÉS\n",
    "params = {\n",
    "    'n_estimators': 437,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.08072903679293161,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8489574811277885,\n",
    "    'colsample_bytree': 0.797160873700788,\n",
    "    'reg_lambda': 0.1572046494326245,\n",
    "    'reg_alpha': 0.03137111664135713,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# C. INITIALISATION ET ENTRAÎNEMENT\n",
    "# On utilise **params pour unpacker le dictionnaire directement\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "print(\"-> Modèle entraîné avec succès (Hyperparamètres chargés).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PRÉDICTIONS ET RÉSULTATS DÉTAILLÉS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CALCUL DES RÉSULTATS DÉTAILLÉS (APPROCHE 1:10:50)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "results = X_test.copy()\n",
    "results['True_Class'] = y_test            \n",
    "results['Prob_Class_0'] = probas[:, 0]    \n",
    "results['Prob_Class_1'] = probas[:, 1]    \n",
    "results['Prob_Class_2'] = probas[:, 2]    \n",
    "\n",
    "# Calculs additionnels\n",
    "results['Score_Risque_GrandFeu'] = results['Prob_Class_2']\n",
    "results['Score_Risque_Percent'] = (results['Score_Risque_GrandFeu'] * 100).round(1)\n",
    "\n",
    "# A. Score Global (AUC)\n",
    "y_test_binary = (y_test == 2).astype(int) \n",
    "auc = roc_auc_score(y_test_binary, results['Prob_Class_2'])\n",
    "print(f\"SCORE DE PERFORMANCE GLOBAL (AUC Grand Feu) : {auc:.3f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# B. Analyse par seuil de 50%\n",
    "print(\"\\nANALYSE DE RISQUE (Seuil > 50%)\")\n",
    "for cl, label in [(0, \"Petits Feux\"), (1, \"Moyens Feux\"), (2, \"Grands Feux\")]:\n",
    "    subset = results[results['True_Class'] == cl]\n",
    "    nb_alert = (subset['Score_Risque_GrandFeu'] > 0.5).sum()\n",
    "    pct_alert = (nb_alert / len(subset)) * 100 if len(subset) > 0 else 0\n",
    "    print(f\"{label:<12} détectés à haut risque : {pct_alert:6.2f}% ({nb_alert}/{len(subset)})\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# C. Table de calibration pour les Grands Feux\n",
    "print_calibration_table_notebook(results, class_id=2, class_name=\"Grands Feux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce5f45",
   "metadata": {},
   "source": [
    "Le resultat que nous avons ici est très bon car il a une auc de 0.893 ce qui est excellent tout en détectant bien les grands feu sans avoir beaucoup de fausse alerte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1217fa7",
   "metadata": {},
   "source": [
    "Nous allons maintenant essayé de tout refaire avec le catboost, cela necessite que nous modifions notre cleaning pour garder les columns catégorielle sans les encoder avec notre fichier catclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0b9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de Nettoyage (Mode CatBoost) - Démarrage. Dimensions : (73221, 308)\n",
      "======================================================================\n",
      "Colonnes avec plus de 50.0% de NaN à supprimer : 23\n",
      "Colonnes constantes à supprimer : 8\n",
      "Total de colonnes supprimées (Step 1) : 24\n",
      "--- DÉBUT DU NETTOYAGE DÉTAILLÉ (handle_aberrant_values) ---\n",
      "   -> 225 colonnes float vérifiées.\n",
      "   -> 92 valeurs techniques (-1e30) remplacées par NaN.\n",
      "   -> 3409 codes d'erreur (< -900) remplacés par NaN.\n",
      "   -> 1474 erreurs topo (32767) remplacées par NaN.\n",
      "Total des cellules corrigées : 4975\n",
      "-> Features temporelles (Mois, Jour) extraites.\n",
      "-> Conversion des colonnes 'faux-textes' en numériques...\n",
      "-> Colonnes catégorielles supprimées (Manuelles) : 15\n",
      "-> Imputation des NaN catégoriels par 'Missing' sur 22 colonnes.\n",
      "Dimensions avant traitement des NaN : (73221, 271)\n",
      "   -> 'DISCOVERY_TIME' transformée en 'Discovery_Time_Hours'.\n",
      "-> Colonnes supprimées (Leakage/IDs) : 4\n",
      "-> Sauvetage des colonnes critiques : ['EVH_1km', 'EVT_1km', 'EVC_1km', 'CH_1km', 'Land_Cover_1km']\n",
      "   -> EVH_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVT_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVC_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> Land_Cover_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "-> Remplissage des valeurs manquantes (Médiane)...\n",
      "   ALERTE : 6 colonnes non-critiques sont entièrement vides.\n",
      "   (Exemples: ['MOD_NDVI_12m', 'MOD_EVI_12m', 'FRG_1km']) -> Suppression.\n",
      "-> NaN restants dans les colonnes numériques : 0\n",
      "-> Création des features avancées (Physique & Clustering)...\n",
      "   -> Interactions physiques ajoutées.\n",
      "   -> Clustering Spatial terminé (50 régions créées).\n",
      "-> Conversion de 21 colonnes en type 'category' pour CatBoost.\n",
      "======================================================================\n",
      "Pipeline terminé. Dimensions finales : (73221, 266)\n",
      "Note : Les colonnes catégorielles sont préservées (dtype='category').\n"
     ]
    }
   ],
   "source": [
    "from catclean import * \n",
    "df_catboost=without_encode(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b06e3",
   "metadata": {},
   "source": [
    "appliquons le modèle et regardons ses résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. PRÉPARATION DES DONNÉES ---\n",
      "Features utilisées : 50\n",
      "Features catégorielles détectées : 7\n",
      "Train size: (58576, 50) | Test size: (14645, 50)\n",
      "\n",
      ">>> STRATÉGIE DE POIDS APPLIQUÉE : [Petit: 1, Moyen: 5, Grand: 20]\n",
      "\n",
      "--- Entraînement du modèle CatBoost ---\n",
      "0:\ttest: 0.6363910\tbest: 0.6363910 (0)\ttotal: 347ms\tremaining: 5m 47s\n",
      "100:\ttest: 0.8305164\tbest: 0.8305164 (99)\ttotal: 18.1s\tremaining: 2m 41s\n",
      "200:\ttest: 0.8504625\tbest: 0.8504625 (200)\ttotal: 39.5s\tremaining: 2m 37s\n",
      "300:\ttest: 0.8537572\tbest: 0.8537572 (300)\ttotal: 1m 4s\tremaining: 2m 29s\n",
      "400:\ttest: 0.8554756\tbest: 0.8556330 (394)\ttotal: 1m 28s\tremaining: 2m 12s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8556329933\n",
      "bestIteration = 394\n",
      "\n",
      "Shrink model to first 395 iterations.\n",
      "\n",
      "==========================================================================================\n",
      " RÉSULTATS DÉTAILLÉS MULTI-CLASSES (OPTIMISÉS SÉCURITÉ)\n",
      "==========================================================================================\n",
      "AUC Global (Weighted) : 0.8621\n",
      "Accuracy Globale      : 0.8264\n",
      "Recall Grand Feu      : 60.14% (Priorité absolue)\n",
      "\n",
      "--- TOP 10 ALERTES (Triées par probabilité de Grand Feu) ---\n",
      "P(Petit)   | P(Moyen)   | P(Grand)   | VRAI   | ÉTAT   | CAUSE\n",
      "------------------------------------------------------------------------------------------\n",
      "0.8%       | 4.1%       | 95.2%      | 1      | TX     | Power generation/tra\n",
      "1.7%       | 3.9%       | 94.5%      | 0      | WA     | Missing data/not spe\n",
      "1.2%       | 4.6%       | 94.2%      | 2      | OK     | Missing data/not spe\n",
      "1.3%       | 4.8%       | 93.9%      | 2      | MT     | Natural\n",
      "1.0%       | 5.1%       | 93.9%      | 0      | TX     | Natural\n",
      "2.0%       | 4.3%       | 93.7%      | 2      | AK     | Natural\n",
      "2.5%       | 4.3%       | 93.3%      | 0      | AK     | Natural\n",
      "1.7%       | 5.1%       | 93.2%      | 0      | NM     | Natural\n",
      "2.8%       | 4.5%       | 92.7%      | 1      | AK     | Natural\n",
      "2.8%       | 4.5%       | 92.7%      | 1      | AK     | Natural\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : PETIT FEU (Classe 0)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Petit Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 950        | 46.6%\n",
      "20-40%               | 1529       | 69.7%\n",
      "40-60%               | 1786       | 84.2%\n",
      "60-80%               | 2945       | 92.4%\n",
      "80-100%              | 7435       | 98.6%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : MOYEN FEU (Classe 1)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Moyen Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 9896       | 2.7%\n",
      "20-40%               | 2984       | 13.6%\n",
      "40-60%               | 1398       | 25.0%\n",
      "60-80%               | 367        | 38.7%\n",
      "80-100%              | 0          | -\n",
      "-----------------------------------------------------------------\n",
      "\n",
      ">>> ANALYSE DE FIABILITÉ : GRAND FEU (Classe 2)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Grand Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 11904      | 0.6%\n",
      "20-40%               | 1638       | 5.4%\n",
      "40-60%               | 643        | 13.4%\n",
      "60-80%               | 364        | 31.9%\n",
      "80-100%              | 96         | 51.0%\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "==========================================================================================\n",
      " ANALYSE DE RISQUE : PROBABILITÉ D'ÊTRE UN GRAND FEU > 50%\n",
      "==========================================================================================\n",
      "Petits Feux avec > 50% de risque Grand Feu :   2.65%  (346/13062)\n",
      "Moyens Feux avec > 50% de risque Grand Feu :  14.46%  (169/1169)\n",
      "Grands Feux avec > 50% de risque Grand Feu :  50.72%  (210/414)\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "# --- 1. CONFIGURATION (Les 50 Variables \"Élues\") ---\n",
    "BEST_FEATURES = [\n",
    "    'Popo_1km', 'STATE', 'Population', 'EVH', 'EVC', 'rpms_1km', 'OWNER_DESCR', \n",
    "    'FOD_ID', 'Discovery_Time_Hours', 'NWCG_GENERAL_CAUSE', 'Region_Cluster', \n",
    "    'EBLR_PFS', 'EALR_PFS', 'No_FireStation_20.0km', 'Wind_x_Dryness', 'Land_Cover', \n",
    "    'EVT', 'EPL_AGE17', 'LATITUDE', 'GHM', 'NWCG_CAUSE_CLASSIFICATION', 'EPL_MOBILE', \n",
    "    'Ecoregion_US_L3CODE', 'NDVI-1day', 'Ecoregion_NA_L2CODE', 'Mang_Type', 'GDP', \n",
    "    'pr_Normal', 'EPL_NOHSDP', 'PM25F_PFS', 'bi_Percentile', 'rmin', 'sph', \n",
    "    'Annual_precipitation', 'HBF_PFS', 'vpd_Normal', 'Slope_x_Wind', 'sph_Normal', \n",
    "    'Slope_1km', 'rmax_Normal', 'bi_Normal', 'TRACT', 'Elevation', 'RPL_THEME4', \n",
    "    'EPLR_PFS', 'MHVF_PFS', 'RPL_THEME2', 'SDI', 'Wind_x_Potential', 'RMP_PFS'\n",
    "]\n",
    "\n",
    "# --- 2. HYPERPARAMÈTRES DE BASE ---\n",
    "BASE_PARAMS = {\n",
    "    'iterations': 1000,          \n",
    "    'learning_rate': 0.094012,   \n",
    "    'depth': 7,                  \n",
    "    'l2_leaf_reg': 7.827,        \n",
    "    'bagging_temperature': 0.703,\n",
    "    'random_strength': 9.302,    \n",
    "    'border_count': 182,         \n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "# --- 3. FONCTION UTILITAIRE (AFFICHAGE) ---\n",
    "def print_calibration_table(df_results, class_id, class_name):\n",
    "    \"\"\" Affiche la table de fiabilité pour une classe donnée \"\"\"\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    \n",
    "    # Création des tranches de probabilités\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    df_results['Tranche'] = pd.cut(df_results[col_score], bins=bins, labels=labels)\n",
    "\n",
    "    print(f\"\\n>>> ANALYSE DE FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    counts = df_results['Tranche'].value_counts(sort=False)\n",
    "    \n",
    "    for label in labels:\n",
    "        if label in counts.index and counts[label] > 0:\n",
    "            subset = df_results[df_results['Tranche'] == label]\n",
    "            real_percentage = (subset['True_Class'] == class_id).mean() * 100\n",
    "            print(f\"{label:<20} | {counts[label]:<10} | {real_percentage:.1f}%\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# ==============================================================================\n",
    "# PRÉPARATION DES DONNÉES (Basé sur df_catboost existant)\n",
    "# ==============================================================================\n",
    "\n",
    "# On travaille sur une copie pour ne pas modifier l'original par erreur\n",
    "df_working = df_catboost.copy()\n",
    "\n",
    "print(\"--- 1. PRÉPARATION DES DONNÉES ---\")\n",
    "\n",
    "# Création de la Cible (Mapping 3 Classes comme dans le script original)\n",
    "mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "\n",
    "if 'FIRE_SIZE_CLASS' in df_working.columns:\n",
    "    df_working = df_working.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "    \n",
    "    # Vérification si le mapping est nécessaire (si les données sont encore des lettres)\n",
    "    val_sample = df_working['FIRE_SIZE_CLASS'].iloc[0]\n",
    "    if isinstance(val_sample, str) and val_sample in mapping:\n",
    "         df_working['FIRE_SIZE_CLASS'] = df_working['FIRE_SIZE_CLASS'].map(mapping)\n",
    "    \n",
    "    # Conversion en entier\n",
    "    df_working['FIRE_SIZE_CLASS'] = df_working['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "# Filtrage pour ne garder que les colonnes existantes parmi les \"BEST_FEATURES\"\n",
    "existing_features = [f for f in BEST_FEATURES if f in df_working.columns]\n",
    "X = df_working[existing_features]\n",
    "y = df_working['FIRE_SIZE_CLASS']\n",
    "\n",
    "# Identification des features catégorielles\n",
    "cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Features utilisées : {len(existing_features)}\")\n",
    "print(f\"Features catégorielles détectées : {len(cat_features)}\")\n",
    "\n",
    "# Split (Stratifié pour garder les proportions initiales)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train size: {X_train.shape} | Test size: {X_test.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ENTRAÎNEMENT & RÉSULTATS\n",
    "# ==============================================================================\n",
    "\n",
    "# DÉFINITION DES POIDS (STRATÉGIE 4 - TRÈS AGRESSIF)\n",
    "selected_weights = [1, 5, 20]\n",
    "print(f\"\\n>>> STRATÉGIE DE POIDS APPLIQUÉE : [Petit: 1, Moyen: 5, Grand: 20]\")\n",
    "\n",
    "print(\"\\n--- Entraînement du modèle CatBoost ---\")\n",
    "\n",
    "# Fusion des paramètres\n",
    "final_params = BASE_PARAMS.copy()\n",
    "final_params['class_weights'] = selected_weights\n",
    "\n",
    "model = CatBoostClassifier(**final_params, cat_features=cat_features)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Prédictions & Scores\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" RÉSULTATS DÉTAILLÉS MULTI-CLASSES (OPTIMISÉS SÉCURITÉ)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "probas = model.predict_proba(X_test)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Scores\n",
    "auc = roc_auc_score(y_test, probas, multi_class='ovr', average='weighted')\n",
    "acc = accuracy_score(y_test, preds)\n",
    "# Recall par classe (le [2] est celui du Grand Feu)\n",
    "recalls = recall_score(y_test, preds, average=None, zero_division=0)\n",
    "\n",
    "print(f\"AUC Global (Weighted) : {auc:.4f}\")\n",
    "print(f\"Accuracy Globale      : {acc:.4f}\")\n",
    "print(f\"Recall Grand Feu      : {recalls[2]:.2%} (Priorité absolue)\")\n",
    "\n",
    "# Préparation du DataFrame d'analyse\n",
    "results = X_test.copy()\n",
    "results['True_Class'] = y_test\n",
    "results['Predicted_Class'] = preds\n",
    "results['Prob_Class_0'] = probas[:, 0] # Petit\n",
    "results['Prob_Class_1'] = probas[:, 1] # Moyen\n",
    "results['Prob_Class_2'] = probas[:, 2] # Grand\n",
    "\n",
    "# --- PARTIE A : TOP 10 DES PLUS GROS RISQUES ---\n",
    "print(\"\\n--- TOP 10 ALERTES (Triées par probabilité de Grand Feu) ---\")\n",
    "top_alerts = results.sort_values(by='Prob_Class_2', ascending=False).head(10)\n",
    "\n",
    "# En-tête avec les 3 probabilités\n",
    "print(f\"{'P(Petit)':<10} | {'P(Moyen)':<10} | {'P(Grand)':<10} | {'VRAI':<6} | {'ÉTAT':<6} | {'CAUSE'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for idx, row in top_alerts.iterrows():\n",
    "    p0 = f\"{row['Prob_Class_0']*100:.1f}%\"\n",
    "    p1 = f\"{row['Prob_Class_1']*100:.1f}%\"\n",
    "    p2 = f\"{row['Prob_Class_2']*100:.1f}%\"\n",
    "    \n",
    "    # Gestion des valeurs manquantes pour l'affichage\n",
    "    cause = str(row.get('NWCG_GENERAL_CAUSE', 'N/A'))[:20]\n",
    "    state = str(row.get('STATE', 'N/A'))\n",
    "    true_c = str(row['True_Class'])\n",
    "    \n",
    "    print(f\"{p0:<10} | {p1:<10} | {p2:<10} | {true_c:<6} | {state:<6} | {cause}\")\n",
    "\n",
    "# --- PARTIE B : ANALYSE DE FIABILITÉ ---\n",
    "print_calibration_table(results, 0, \"Petit Feu\")\n",
    "print_calibration_table(results, 1, \"Moyen Feu\")\n",
    "print_calibration_table(results, 2, \"Grand Feu\")\n",
    "\n",
    "# --- PARTIE C : ANALYSE DE RISQUE (SEUIL 50% GRAND FEU) ---\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" ANALYSE DE RISQUE : PROBABILITÉ D'ÊTRE UN GRAND FEU > 50%\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# 1. Pour les PETITS feux (True_Class == 0)\n",
    "subset_petit = results[results['True_Class'] == 0]\n",
    "nb_petit_alert = (subset_petit['Prob_Class_2'] > 0.5).sum()\n",
    "pct_petit_alert = (nb_petit_alert / len(subset_petit)) * 100 if len(subset_petit) > 0 else 0\n",
    "print(f\"Petits Feux avec > 50% de risque Grand Feu : {pct_petit_alert:6.2f}%  ({nb_petit_alert}/{len(subset_petit)})\")\n",
    "\n",
    "# 2. Pour les MOYENS feux (True_Class == 1)\n",
    "subset_moyen = results[results['True_Class'] == 1]\n",
    "nb_moyen_alert = (subset_moyen['Prob_Class_2'] > 0.5).sum()\n",
    "pct_moyen_alert = (nb_moyen_alert / len(subset_moyen)) * 100 if len(subset_moyen) > 0 else 0\n",
    "print(f\"Moyens Feux avec > 50% de risque Grand Feu : {pct_moyen_alert:6.2f}%  ({nb_moyen_alert}/{len(subset_moyen)})\")\n",
    "\n",
    "# 3. Pour les GRANDS feux (True_Class == 2) - C'est le Recall au seuil 0.5\n",
    "subset_grand = results[results['True_Class'] == 2]\n",
    "nb_grand_alert = (subset_grand['Prob_Class_2'] > 0.5).sum()\n",
    "pct_grand_alert = (nb_grand_alert / len(subset_grand)) * 100 if len(subset_grand) > 0 else 0\n",
    "print(f\"Grands Feux avec > 50% de risque Grand Feu : {pct_grand_alert:6.2f}%  ({nb_grand_alert}/{len(subset_grand)})\")\n",
    "print(\"-\" * 90)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8a070",
   "metadata": {},
   "source": [
    "On a avec ce modèle, le catboost des très bon résultat, une AUC moins bonne que sur les deux modèles précedents 0.86 mais une detection des grands feu qui est bien meilleur avec un recall des grands feu à 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411d4c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHARGEMENT ET NETTOYAGE (2020_FPA_FOD_cons.csv) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axjui\\AppData\\Local\\Temp\\ipykernel_20032\\1220696780.py:53: DtypeWarning: Columns (14,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de Nettoyage (Mode CatBoost) - Démarrage. Dimensions : (73221, 308)\n",
      "======================================================================\n",
      "Colonnes avec plus de 50.0% de NaN à supprimer : 23\n",
      "Colonnes constantes à supprimer : 8\n",
      "Total de colonnes supprimées (Step 1) : 24\n",
      "--- DÉBUT DU NETTOYAGE DÉTAILLÉ (handle_aberrant_values) ---\n",
      "   -> 225 colonnes float vérifiées.\n",
      "   -> 92 valeurs techniques (-1e30) remplacées par NaN.\n",
      "   -> 3409 codes d'erreur (< -900) remplacés par NaN.\n",
      "   -> 1474 erreurs topo (32767) remplacées par NaN.\n",
      "Total des cellules corrigées : 4975\n",
      "-> Features temporelles (Mois, Jour) extraites.\n",
      "-> Conversion des colonnes 'faux-textes' en numériques...\n",
      "-> Colonnes catégorielles supprimées (Manuelles) : 15\n",
      "-> Imputation des NaN catégoriels par 'Missing' sur 22 colonnes.\n",
      "Dimensions avant traitement des NaN : (73221, 271)\n",
      "   -> 'DISCOVERY_TIME' transformée en 'Discovery_Time_Hours'.\n",
      "-> Colonnes supprimées (Leakage/IDs) : 4\n",
      "-> Sauvetage des colonnes critiques : ['EVH_1km', 'EVT_1km', 'EVC_1km', 'CH_1km', 'Land_Cover_1km']\n",
      "   -> EVH_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVT_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVC_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> Land_Cover_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "-> Remplissage des valeurs manquantes (Médiane)...\n",
      "   ALERTE : 6 colonnes non-critiques sont entièrement vides.\n",
      "   (Exemples: ['MOD_NDVI_12m', 'MOD_EVI_12m', 'FRG_1km']) -> Suppression.\n",
      "-> NaN restants dans les colonnes numériques : 0\n",
      "-> Création des features avancées (Physique & Clustering)...\n",
      "   -> Interactions physiques ajoutées.\n",
      "   -> Clustering Spatial terminé (50 régions créées).\n",
      "-> Conversion de 21 colonnes en type 'category' pour CatBoost.\n",
      "======================================================================\n",
      "Pipeline terminé. Dimensions finales : (73221, 266)\n",
      "Note : Les colonnes catégorielles sont préservées (dtype='category').\n",
      "\n",
      ">>> CONFIGURATION TRIAL 12\n",
      "Poids appliqués : [1, 5.48189108046529, 70.3743330902225]\n",
      "Profondeur Arbre: 4\n",
      "Learning Rate   : 0.01212\n",
      "\n",
      "--- Entraînement du modèle (Trial 12) ---\n",
      "0:\ttest: 0.5840567\tbest: 0.5840567 (0)\ttotal: 88.6ms\tremaining: 1m 28s\n",
      "100:\ttest: 0.8013262\tbest: 0.8013262 (100)\ttotal: 7.61s\tremaining: 1m 7s\n",
      "200:\ttest: 0.8097436\tbest: 0.8097763 (199)\ttotal: 17.1s\tremaining: 1m 7s\n",
      "300:\ttest: 0.8145134\tbest: 0.8146288 (289)\ttotal: 27.9s\tremaining: 1m 4s\n",
      "400:\ttest: 0.8179977\tbest: 0.8179977 (400)\ttotal: 38s\tremaining: 56.8s\n",
      "500:\ttest: 0.8207598\tbest: 0.8207822 (499)\ttotal: 48s\tremaining: 47.8s\n",
      "600:\ttest: 0.8234055\tbest: 0.8234055 (600)\ttotal: 58.6s\tremaining: 38.9s\n",
      "700:\ttest: 0.8254339\tbest: 0.8254339 (700)\ttotal: 1m 8s\tremaining: 29.1s\n",
      "800:\ttest: 0.8271930\tbest: 0.8272201 (799)\ttotal: 1m 18s\tremaining: 19.5s\n",
      "900:\ttest: 0.8286458\tbest: 0.8286458 (900)\ttotal: 1m 28s\tremaining: 9.71s\n",
      "999:\ttest: 0.8307893\tbest: 0.8307893 (999)\ttotal: 1m 38s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8307893267\n",
      "bestIteration = 999\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      " RÉSULTATS DÉTAILLÉS - CONFIGURATION TRIAL 12\n",
      "==========================================================================================\n",
      "AUC Global (Weighted) : 0.8119\n",
      "Recall Petit Feu      : 64.36%\n",
      "Recall Moyen Feu      : 2.65%\n",
      "Recall Grand Feu      : 93.72%  <-- SCORE CIBLE (OPTIMISÉ)\n",
      "\n",
      "Matrice de Confusion (Lignes=Vrai, Colonnes=Prédictions) :\n",
      "[[8407   75 4580]\n",
      " [ 263   31  875]\n",
      " [  26    0  388]]\n",
      "\n",
      "--- TOP 10 ALERTES (Triées par probabilité de Grand Feu) ---\n",
      "P(Petit)   | P(Moyen)   | P(Grand)   | VRAI   | ÉTAT   | CAUSE\n",
      "------------------------------------------------------------------------------------------\n",
      "2.4%      | 4.4%      | 93.2%      | 0      | AK     | Natural\n",
      "2.6%      | 4.4%      | 92.9%      | 2      | AK     | Natural\n",
      "2.5%      | 4.6%      | 92.9%      | 0      | AK     | Natural\n",
      "2.5%      | 4.6%      | 92.9%      | 0      | AK     | Natural\n",
      "2.5%      | 4.6%      | 92.9%      | 0      | AK     | Natural\n",
      "2.2%      | 5.0%      | 92.8%      | 2      | AK     | Natural\n",
      "2.6%      | 4.6%      | 92.8%      | 1      | AK     | Natural\n",
      "2.6%      | 4.6%      | 92.8%      | 1      | AK     | Natural\n",
      "2.6%      | 4.7%      | 92.7%      | 0      | AK     | Natural\n",
      "2.2%      | 5.1%      | 92.7%      | 0      | AK     | Natural\n",
      "\n",
      ">>> FIABILITÉ : GRAND FEU (Classe 2)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Grand Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 4670       | 0.1%\n",
      "20-40%               | 4021       | 0.6%\n",
      "40-60%               | 2806       | 2.3%\n",
      "60-80%               | 2371       | 6.6%\n",
      "80-100%              | 777        | 21.2%\n",
      "\n",
      "--- FAUSSES ALERTES ---\n",
      "Nombre de Petits Feux classés comme 'Grands' : 4580\n",
      "Taux de fausses alertes sur les Petits Feux : 35.06%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- Import du nettoyage ---\n",
    "try:\n",
    "    from catclean import without_encode\n",
    "except ImportError:\n",
    "    # Fallback si le fichier n'est pas là pour le test\n",
    "    def without_encode(df): return df\n",
    "    print(\"ATTENTION : 'catclean.py' introuvable. Mode dégradé.\")\n",
    "\n",
    "# --- 1. CONFIGURATION DES FEATURES ---\n",
    "BEST_FEATURES = [\n",
    "    'Popo_1km', 'STATE', 'Population', 'EVH', 'EVC', 'rpms_1km', 'OWNER_DESCR', \n",
    "    'FOD_ID', 'Discovery_Time_Hours', 'NWCG_GENERAL_CAUSE', 'Region_Cluster', \n",
    "    'EBLR_PFS', 'EALR_PFS', 'No_FireStation_20.0km', 'Wind_x_Dryness', 'Land_Cover', \n",
    "    'EVT', 'EPL_AGE17', 'LATITUDE', 'GHM', 'NWCG_CAUSE_CLASSIFICATION', 'EPL_MOBILE', \n",
    "    'Ecoregion_US_L3CODE', 'NDVI-1day', 'Ecoregion_NA_L2CODE', 'Mang_Type', 'GDP', \n",
    "    'pr_Normal', 'EPL_NOHSDP', 'PM25F_PFS', 'bi_Percentile', 'rmin', 'sph', \n",
    "    'Annual_precipitation', 'HBF_PFS', 'vpd_Normal', 'Slope_x_Wind', 'sph_Normal', \n",
    "    'Slope_1km', 'rmax_Normal', 'bi_Normal', 'TRACT', 'Elevation', 'RPL_THEME4', \n",
    "    'EPLR_PFS', 'MHVF_PFS', 'RPL_THEME2', 'SDI', 'Wind_x_Potential', 'RMP_PFS'\n",
    "]\n",
    "\n",
    "# --- 2. PARAMÈTRES DU TRIAL 12 ---\n",
    "# Poids extraits : Petit=1 (fixe), Moyen=5.48, Grand=70.37\n",
    "TRIAL_12_WEIGHTS = [1, 5.48189108046529, 70.3743330902225]\n",
    "\n",
    "TRIAL_12_PARAMS = {\n",
    "    'iterations': 1000,          \n",
    "    'learning_rate': 0.01211508050413514,   \n",
    "    'depth': 4,                  \n",
    "    'l2_leaf_reg': 1.93541594822678,        \n",
    "    'bagging_temperature': 0.015055503993032683,\n",
    "    'random_strength': 7.431601452326245,    \n",
    "    'border_count': 209,         \n",
    "    \n",
    "    # Paramètres fixes\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'verbose': 100,\n",
    "    'class_weights': TRIAL_12_WEIGHTS\n",
    "}\n",
    "\n",
    "def get_data_prepared(filename):\n",
    "    print(f\"--- CHARGEMENT ET NETTOYAGE ({filename}) ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR : '{filename}' introuvable.\")\n",
    "        exit()\n",
    "\n",
    "    df_clean = without_encode(df)\n",
    "\n",
    "    mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "    if 'FIRE_SIZE_CLASS' in df_clean.columns:\n",
    "        df_clean = df_clean.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "        val_sample = df_clean['FIRE_SIZE_CLASS'].iloc[0]\n",
    "        if isinstance(val_sample, str) and val_sample in mapping:\n",
    "             df_clean['FIRE_SIZE_CLASS'] = df_clean['FIRE_SIZE_CLASS'].map(mapping)\n",
    "        df_clean['FIRE_SIZE_CLASS'] = df_clean['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "    existing_features = [f for f in BEST_FEATURES if f in df_clean.columns]\n",
    "    X = df_clean[existing_features]\n",
    "    y = df_clean['FIRE_SIZE_CLASS']\n",
    "    return X, y\n",
    "\n",
    "def print_calibration_table(df_results, class_id, class_name):\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    df_results['Tranche'] = pd.cut(df_results[col_score], bins=bins, labels=labels)\n",
    "\n",
    "    print(f\"\\n>>> FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    counts = df_results['Tranche'].value_counts(sort=False)\n",
    "    for label in labels:\n",
    "        if label in counts.index and counts[label] > 0:\n",
    "            subset = df_results[df_results['Tranche'] == label]\n",
    "            real_percentage = (subset['True_Class'] == class_id).mean() * 100\n",
    "            print(f\"{label:<20} | {counts[label]:<10} | {real_percentage:.1f}%\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PROGRAM\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"2020_FPA_FOD_cons.csv\"\n",
    "    X, y = get_data_prepared(filename)\n",
    "\n",
    "    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Split Stratifié\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n>>> CONFIGURATION TRIAL 12\")\n",
    "    print(f\"Poids appliqués : {TRIAL_12_WEIGHTS}\")\n",
    "    print(f\"Profondeur Arbre: {TRIAL_12_PARAMS['depth']}\")\n",
    "    print(f\"Learning Rate   : {TRIAL_12_PARAMS['learning_rate']:.5f}\")\n",
    "\n",
    "    # Entraînement\n",
    "    print(\"\\n--- Entraînement du modèle (Trial 12) ---\")\n",
    "    model = CatBoostClassifier(**TRIAL_12_PARAMS, cat_features=cat_features)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    # Prédictions\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\" RÉSULTATS DÉTAILLÉS - CONFIGURATION TRIAL 12\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    probas = model.predict_proba(X_test)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Scores\n",
    "    auc = roc_auc_score(y_test, probas, multi_class='ovr', average='weighted')\n",
    "    recalls = recall_score(y_test, preds, average=None, zero_division=0)\n",
    "    \n",
    "    print(f\"AUC Global (Weighted) : {auc:.4f}\")\n",
    "    print(f\"Recall Petit Feu      : {recalls[0]:.2%}\")\n",
    "    print(f\"Recall Moyen Feu      : {recalls[1]:.2%}\")\n",
    "    print(f\"Recall Grand Feu      : {recalls[2]:.2%}  <-- SCORE CIBLE (OPTIMISÉ)\")\n",
    "\n",
    "    # Matrice de confusion simplifiée\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"\\nMatrice de Confusion (Lignes=Vrai, Colonnes=Prédictions) :\")\n",
    "    print(cm)\n",
    "\n",
    "    # Analyse détaillée\n",
    "    results = X_test.copy()\n",
    "    results['True_Class'] = y_test\n",
    "    results['Predicted_Class'] = preds\n",
    "    results['Prob_Class_0'] = probas[:, 0]\n",
    "    results['Prob_Class_1'] = probas[:, 1]\n",
    "    results['Prob_Class_2'] = probas[:, 2]\n",
    "\n",
    "    # --- TOP 10 ALERTES ---\n",
    "    print(\"\\n--- TOP 10 ALERTES (Triées par probabilité de Grand Feu) ---\")\n",
    "    top_alerts = results.sort_values(by='Prob_Class_2', ascending=False).head(10)\n",
    "    print(f\"{'P(Petit)':<10} | {'P(Moyen)':<10} | {'P(Grand)':<10} | {'VRAI':<6} | {'ÉTAT':<6} | {'CAUSE'}\")\n",
    "    print(\"-\" * 90)\n",
    "    for idx, row in top_alerts.iterrows():\n",
    "        p0, p1, p2 = row['Prob_Class_0'], row['Prob_Class_1'], row['Prob_Class_2']\n",
    "        cause = str(row['NWCG_GENERAL_CAUSE'])[:20]\n",
    "        state = str(row['STATE'])\n",
    "        true_c = str(row['True_Class'])\n",
    "        print(f\"{p0:.1%}      | {p1:.1%}      | {p2:.1%}      | {true_c:<6} | {state:<6} | {cause}\")\n",
    "\n",
    "    # --- FIABILITÉ ---\n",
    "    print_calibration_table(results, 2, \"Grand Feu\")\n",
    "\n",
    "    # --- ANALYSE DE RISQUE ---\n",
    "    print(\"\\n--- FAUSSES ALERTES ---\")\n",
    "    # Combien de fois a-t-on prédit Grand Feu alors que c'était un Petit Feu ?\n",
    "    false_alarms = results[(results['Predicted_Class'] == 2) & (results['True_Class'] == 0)]\n",
    "    print(f\"Nombre de Petits Feux classés comme 'Grands' : {len(false_alarms)}\")\n",
    "    if len(results[results['True_Class'] == 0]) > 0:\n",
    "        rate = len(false_alarms) / len(results[results['True_Class'] == 0])\n",
    "        print(f\"Taux de fausses alertes sur les Petits Feux : {rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c334f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHARGEMENT (2020_FPA_FOD_cons.csv) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axjui\\AppData\\Local\\Temp\\ipykernel_20032\\794479482.py:54: DtypeWarning: Columns (14,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de Nettoyage (Mode CatBoost) - Démarrage. Dimensions : (73221, 308)\n",
      "======================================================================\n",
      "Colonnes avec plus de 50.0% de NaN à supprimer : 23\n",
      "Colonnes constantes à supprimer : 8\n",
      "Total de colonnes supprimées (Step 1) : 24\n",
      "--- DÉBUT DU NETTOYAGE DÉTAILLÉ (handle_aberrant_values) ---\n",
      "   -> 225 colonnes float vérifiées.\n",
      "   -> 92 valeurs techniques (-1e30) remplacées par NaN.\n",
      "   -> 3409 codes d'erreur (< -900) remplacés par NaN.\n",
      "   -> 1474 erreurs topo (32767) remplacées par NaN.\n",
      "Total des cellules corrigées : 4975\n",
      "-> Features temporelles (Mois, Jour) extraites.\n",
      "-> Conversion des colonnes 'faux-textes' en numériques...\n",
      "-> Colonnes catégorielles supprimées (Manuelles) : 15\n",
      "-> Imputation des NaN catégoriels par 'Missing' sur 22 colonnes.\n",
      "Dimensions avant traitement des NaN : (73221, 271)\n",
      "   -> 'DISCOVERY_TIME' transformée en 'Discovery_Time_Hours'.\n",
      "-> Colonnes supprimées (Leakage/IDs) : 4\n",
      "-> Sauvetage des colonnes critiques : ['EVH_1km', 'EVT_1km', 'EVC_1km', 'CH_1km', 'Land_Cover_1km']\n",
      "   -> EVH_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVT_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> EVC_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "   -> Land_Cover_1km : 73221 NaN remplacés par -1.0 (Sauvegardée).\n",
      "-> Remplissage des valeurs manquantes (Médiane)...\n",
      "   ALERTE : 6 colonnes non-critiques sont entièrement vides.\n",
      "   (Exemples: ['MOD_NDVI_12m', 'MOD_EVI_12m', 'FRG_1km']) -> Suppression.\n",
      "-> NaN restants dans les colonnes numériques : 0\n",
      "-> Création des features avancées (Physique & Clustering)...\n",
      "   -> Interactions physiques ajoutées.\n",
      "   -> Clustering Spatial terminé (50 régions créées).\n",
      "-> Conversion de 21 colonnes en type 'category' pour CatBoost.\n",
      "======================================================================\n",
      "Pipeline terminé. Dimensions finales : (73221, 266)\n",
      "Note : Les colonnes catégorielles sont préservées (dtype='category').\n",
      "\n",
      ">>> CONFIGURATION V2 (LONG RUN)\n",
      "Poids appliqués : [1, 12, 70] (Moyen Feu boosté)\n",
      "Iterations      : 3000 (Pour convergence)\n",
      "\n",
      "--- Entraînement en cours (patience requise...) ---\n",
      "0:\ttest: 0.5881275\tbest: 0.5881275 (0)\ttotal: 96.1ms\tremaining: 4m 48s\n",
      "200:\ttest: 0.8108341\tbest: 0.8109695 (198)\ttotal: 19.7s\tremaining: 4m 33s\n",
      "400:\ttest: 0.8192345\tbest: 0.8192345 (400)\ttotal: 43.2s\tremaining: 4m 40s\n",
      "600:\ttest: 0.8231176\tbest: 0.8231176 (600)\ttotal: 1m 7s\tremaining: 4m 29s\n",
      "800:\ttest: 0.8265114\tbest: 0.8265114 (800)\ttotal: 1m 28s\tremaining: 4m 4s\n",
      "1000:\ttest: 0.8301102\tbest: 0.8301102 (1000)\ttotal: 1m 51s\tremaining: 3m 42s\n",
      "1200:\ttest: 0.8381135\tbest: 0.8381135 (1200)\ttotal: 2m 17s\tremaining: 3m 25s\n",
      "1400:\ttest: 0.8437106\tbest: 0.8437106 (1400)\ttotal: 2m 41s\tremaining: 3m 4s\n",
      "1600:\ttest: 0.8463869\tbest: 0.8463924 (1597)\ttotal: 3m 4s\tremaining: 2m 40s\n",
      "1800:\ttest: 0.8480842\tbest: 0.8480842 (1800)\ttotal: 3m 27s\tremaining: 2m 18s\n",
      "2000:\ttest: 0.8496158\tbest: 0.8496158 (2000)\ttotal: 3m 50s\tremaining: 1m 55s\n",
      "2200:\ttest: 0.8505311\tbest: 0.8505474 (2198)\ttotal: 4m 14s\tremaining: 1m 32s\n",
      "2400:\ttest: 0.8512218\tbest: 0.8512280 (2398)\ttotal: 4m 37s\tremaining: 1m 9s\n",
      "2600:\ttest: 0.8522062\tbest: 0.8522360 (2569)\ttotal: 5m\tremaining: 46.1s\n",
      "2800:\ttest: 0.8532294\tbest: 0.8532329 (2797)\ttotal: 5m 23s\tremaining: 23s\n",
      "2999:\ttest: 0.8541469\tbest: 0.8541745 (2993)\ttotal: 5m 47s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8541745024\n",
      "bestIteration = 2993\n",
      "\n",
      "Shrink model to first 2994 iterations.\n",
      "\n",
      "==========================================================================================\n",
      " RÉSULTATS DÉTAILLÉS - MODELE V2\n",
      "==========================================================================================\n",
      "AUC Global (Weighted) : 0.8471\n",
      "Recall Petit Feu      : 67.46%\n",
      "Recall Moyen Feu      : 43.63%  (Doit augmenter)\n",
      "Recall Grand Feu      : 82.13%  (Doit rester haut)\n",
      "\n",
      "Matrice de Confusion (Lignes=Vrai, Colonnes=Prédictions) :\n",
      "[[8811 2003 2248]\n",
      " [ 194  510  465]\n",
      " [  30   44  340]]\n",
      "\n",
      ">>> FIABILITÉ : GRAND FEU (Classe 2)\n",
      "Tranche de Proba     | Nb Feux    | % Réellement Grand Feu\n",
      "-----------------------------------------------------------------\n",
      "0-20%                | 8724       | 0.2%\n",
      "20-40%               | 2779       | 2.1%\n",
      "40-60%               | 1702       | 4.2%\n",
      "60-80%               | 1125       | 14.0%\n",
      "80-100%              | 315        | 35.6%\n",
      "\n",
      "--- FAUSSES ALERTES (Pression sur les secours) ---\n",
      "Petits Feux classés 'Grands' : 2248 / 13062\n",
      "Taux de fausses alertes      : 17.21% (Cible : < 35%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Import du nettoyage ---\n",
    "try:\n",
    "    from catclean import without_encode\n",
    "except ImportError:\n",
    "    print(\"ATTENTION : 'catclean.py' introuvable. Mode dégradé.\")\n",
    "    def without_encode(df): return df\n",
    "\n",
    "# --- 1. LES 50 VARIABLES ÉLUES ---\n",
    "BEST_FEATURES = [\n",
    "    'Popo_1km', 'STATE', 'Population', 'EVH', 'EVC', 'rpms_1km', 'OWNER_DESCR', \n",
    "    'FOD_ID', 'Discovery_Time_Hours', 'NWCG_GENERAL_CAUSE', 'Region_Cluster', \n",
    "    'EBLR_PFS', 'EALR_PFS', 'No_FireStation_20.0km', 'Wind_x_Dryness', 'Land_Cover', \n",
    "    'EVT', 'EPL_AGE17', 'LATITUDE', 'GHM', 'NWCG_CAUSE_CLASSIFICATION', 'EPL_MOBILE', \n",
    "    'Ecoregion_US_L3CODE', 'NDVI-1day', 'Ecoregion_NA_L2CODE', 'Mang_Type', 'GDP', \n",
    "    'pr_Normal', 'EPL_NOHSDP', 'PM25F_PFS', 'bi_Percentile', 'rmin', 'sph', \n",
    "    'Annual_precipitation', 'HBF_PFS', 'vpd_Normal', 'Slope_x_Wind', 'sph_Normal', \n",
    "    'Slope_1km', 'rmax_Normal', 'bi_Normal', 'TRACT', 'Elevation', 'RPL_THEME4', \n",
    "    'EPLR_PFS', 'MHVF_PFS', 'RPL_THEME2', 'SDI', 'Wind_x_Potential', 'RMP_PFS'\n",
    "]\n",
    "\n",
    "# --- 2. NOUVELLE CONFIGURATION (V2 - CONVERGENCE & ÉQUILIBRE) ---\n",
    "# Poids modifiés : Petit=1, Moyen=12 (Boosté), Grand=70 (Sécurité max)\n",
    "NEW_WEIGHTS = [1, 12, 70]\n",
    "\n",
    "PARAMS_V2 = {\n",
    "    'iterations': 3000,             # AUGMENTÉ (x3) pour laisser finir l'apprentissage\n",
    "    'learning_rate': 0.012115,      # On garde ce LR lent et précis\n",
    "    'depth': 4,                     # On garde la profondeur 4 (évite l'overfitting)\n",
    "    'l2_leaf_reg': 1.935,\n",
    "    'bagging_temperature': 0.015,\n",
    "    'random_strength': 7.43,\n",
    "    'border_count': 209,\n",
    "    \n",
    "    # Paramètres fixes\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'AUC',\n",
    "    'random_seed': 42,\n",
    "    'allow_writing_files': False,\n",
    "    'verbose': 200,                 # Affiche les logs toutes les 200 itérations\n",
    "    'class_weights': NEW_WEIGHTS,\n",
    "    'od_type': 'Iter',              # Overfitting Detector\n",
    "    'od_wait': 200                  # S'arrête si pas d'amélioration pendant 200 itérations\n",
    "}\n",
    "\n",
    "def get_data_prepared(filename):\n",
    "    print(f\"--- CHARGEMENT ({filename}) ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR : '{filename}' introuvable.\")\n",
    "        exit()\n",
    "\n",
    "    df_clean = without_encode(df)\n",
    "\n",
    "    mapping = {'A': 0, 'B': 0, 'C': 1, 'D': 2, 'E': 2, 'F': 2, 'G': 2}\n",
    "    if 'FIRE_SIZE_CLASS' in df_clean.columns:\n",
    "        df_clean = df_clean.dropna(subset=['FIRE_SIZE_CLASS'])\n",
    "        val = df_clean['FIRE_SIZE_CLASS'].iloc[0]\n",
    "        if isinstance(val, str) and val in mapping:\n",
    "             df_clean['FIRE_SIZE_CLASS'] = df_clean['FIRE_SIZE_CLASS'].map(mapping)\n",
    "        df_clean['FIRE_SIZE_CLASS'] = df_clean['FIRE_SIZE_CLASS'].astype(int)\n",
    "\n",
    "    existing_features = [f for f in BEST_FEATURES if f in df_clean.columns]\n",
    "    X = df_clean[existing_features]\n",
    "    y = df_clean['FIRE_SIZE_CLASS']\n",
    "    return X, y\n",
    "\n",
    "def print_calibration_table(df_results, class_id, class_name):\n",
    "    col_score = f'Prob_Class_{class_id}'\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "    df_results['Tranche'] = pd.cut(df_results[col_score], bins=bins, labels=labels)\n",
    "\n",
    "    print(f\"\\n>>> FIABILITÉ : {class_name.upper()} (Classe {class_id})\")\n",
    "    print(f\"{'Tranche de Proba':<20} | {'Nb Feux':<10} | {'% Réellement ' + class_name}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    counts = df_results['Tranche'].value_counts(sort=False)\n",
    "    for label in labels:\n",
    "        if label in counts.index and counts[label] > 0:\n",
    "            subset = df_results[df_results['Tranche'] == label]\n",
    "            real_percentage = (subset['True_Class'] == class_id).mean() * 100\n",
    "            print(f\"{label:<20} | {counts[label]:<10} | {real_percentage:.1f}%\")\n",
    "        else:\n",
    "            print(f\"{label:<20} | 0          | -\")\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PROGRAM\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"2020_FPA_FOD_cons.csv\"\n",
    "    X, y = get_data_prepared(filename)\n",
    "\n",
    "    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Split Stratifié\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n>>> CONFIGURATION V2 (LONG RUN)\")\n",
    "    print(f\"Poids appliqués : {NEW_WEIGHTS} (Moyen Feu boosté)\")\n",
    "    print(f\"Iterations      : {PARAMS_V2['iterations']} (Pour convergence)\")\n",
    "\n",
    "    # Entraînement\n",
    "    print(\"\\n--- Entraînement en cours (patience requise...) ---\")\n",
    "    model = CatBoostClassifier(**PARAMS_V2, cat_features=cat_features)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test)\n",
    "        # early_stopping géré via od_wait dans les params\n",
    "    )\n",
    "\n",
    "    # Prédictions\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\" RÉSULTATS DÉTAILLÉS - MODELE V2\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    probas = model.predict_proba(X_test)\n",
    "    \n",
    "    # Scores\n",
    "    auc = roc_auc_score(y_test, probas, multi_class='ovr', average='weighted')\n",
    "    recalls = recall_score(y_test, preds, average=None, zero_division=0)\n",
    "    \n",
    "    print(f\"AUC Global (Weighted) : {auc:.4f}\")\n",
    "    print(f\"Recall Petit Feu      : {recalls[0]:.2%}\")\n",
    "    print(f\"Recall Moyen Feu      : {recalls[1]:.2%}  (Doit augmenter)\")\n",
    "    print(f\"Recall Grand Feu      : {recalls[2]:.2%}  (Doit rester haut)\")\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"\\nMatrice de Confusion (Lignes=Vrai, Colonnes=Prédictions) :\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Analyse détaillée pour calibration\n",
    "    results = X_test.copy()\n",
    "    results['True_Class'] = y_test\n",
    "    results['Predicted_Class'] = preds.flatten()\n",
    "    results['Prob_Class_2'] = probas[:, 2]\n",
    "\n",
    "    # --- FIABILITÉ ---\n",
    "    print_calibration_table(results, 2, \"Grand Feu\")\n",
    "\n",
    "    # --- ANALYSE DE RISQUE (Fausses Alertes) ---\n",
    "    print(\"\\n--- FAUSSES ALERTES (Pression sur les secours) ---\")\n",
    "    false_alarms = results[(results['Predicted_Class'] == 2) & (results['True_Class'] == 0)]\n",
    "    nb_petits = len(results[results['True_Class'] == 0])\n",
    "    rate = len(false_alarms) / nb_petits if nb_petits > 0 else 0\n",
    "    \n",
    "    print(f\"Petits Feux classés 'Grands' : {len(false_alarms)} / {nb_petits}\")\n",
    "    print(f\"Taux de fausses alertes      : {rate:.2%} (Cible : < 35%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
