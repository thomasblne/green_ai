{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61653772-4f36-44f8-9f12-24e628a6a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/56/b0/e3efafd9c97ed931f6453bd71aa8feaffc9217e6121af65fda06cf32f608/xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/fd726323-8f75-4d97-86f2-25199d46b933/.local/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12 from https://files.pythonhosted.org/packages/73/61/fa7a709b3f2d57038d99c220eba816b21466567835e4d46300ff674ed975/nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: scipy in /home/fd726323-8f75-4d97-86f2-25199d46b933/.local/lib/python3.11/site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
      "Successfully installed nvidia-nccl-cu12-2.28.7 xgboost-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d05427-e5cf-481c-bcaf-684d60450afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ElasticNet (baseline Green AI) ===\n",
      "RMSE log : 1.185545844170466\n",
      "MAE log  : 0.9236331566428228\n",
      "R² log   : 0.028502361769987328\n",
      "\n",
      "=== XGBoost (log) ===\n",
      "RMSE log : 1.0717072613571375\n",
      "MAE log  : 0.8184621773381953\n",
      "R² log   : 0.2061153757002816\n",
      "\n",
      "=== XGBoost (surface réelle) ===\n",
      "RMSE réel : 54.4028181367064\n",
      "MAE réel  : 10.245447979593221\n",
      "\n",
      "=== Top 15 features (XGBoost) ===\n",
      "cat__Ecoregion_US_L3CODE_45.0            -> 0.0390\n",
      "cat__Ecoregion_US_L3CODE_25.0            -> 0.0118\n",
      "cat__Ecoregion_US_L4CODE_65f             -> 0.0118\n",
      "cat__Ecoregion_US_L3CODE_39.0            -> 0.0114\n",
      "cat__Ecoregion_US_L3CODE_66.0            -> 0.0093\n",
      "cat__Ecoregion_US_L3CODE_84.0            -> 0.0089\n",
      "cat__Ecoregion_US_L4CODE_66g             -> 0.0065\n",
      "cat__Ecoregion_US_L4CODE_84b             -> 0.0064\n",
      "cat__Ecoregion_US_L3CODE_36.0            -> 0.0059\n",
      "cat__Ecoregion_US_L4CODE_65d             -> 0.0057\n",
      "cat__EVT_7997.0                          -> 0.0056\n",
      "cat__Ecoregion_US_L4CODE_69d             -> 0.0056\n",
      "cat__Ecoregion_US_L3CODE_67.0            -> 0.0053\n",
      "cat__EVT_7364.0                          -> 0.0052\n",
      "cat__Ecoregion_US_L4CODE_81j             -> 0.0050\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#  PROJET GREEN AI - PREDICTION FIRE_SIZE\n",
    "#  Modélisation \"simple mais maîtrisée\"\n",
    "# ===========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Chargement des données\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"1992_FPA_FOD_cons.csv\", low_memory=False)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Feature engineering spécifique : NDVI_mean_scalar\n",
    "#    NDVI_mean est une liste de valeurs ('0.1' '0.07' ...)\n",
    "#    -> on la convertit en VRAI NDVI moyen (float)\n",
    "# ----------------------------------------------------------\n",
    "def parse_ndvi_cell(s):\n",
    "    \"\"\"Convertit une chaîne de type \n",
    "    `'0.1' '0.07' '0.19' ...` en moyenne numérique.\n",
    "    Retourne NaN si rien n'est exploitable.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    parts = s.replace(\"'\", \" \").split()\n",
    "    vals = []\n",
    "    for p in parts:\n",
    "        try:\n",
    "            vals.append(float(p))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if not vals:\n",
    "        return np.nan\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "df[\"NDVI_mean_scalar\"] = df[\"NDVI_mean\"].apply(parse_ndvi_cell)\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# 3. Sélection des variables utiles\n",
    "# ------------------------------------\n",
    "# Variables numériques : météo, indices de danger, moyennes 5j, topographie, NDVI\n",
    "features_numeric = [\n",
    "    # météo instantanée\n",
    "    \"pr\", \"tmmn\", \"tmmx\", \"rmin\", \"rmax\", \"sph\", \"vs\", \"th\", \"srad\", \"etr\",\n",
    "    # indices de danger\n",
    "    \"fm100\", \"fm1000\", \"bi\", \"vpd\", \"erc\",\n",
    "    # moyennes glissantes 5 jours\n",
    "    \"pr_5D_mean\", \"tmmn_5D_mean\", \"tmmx_5D_mean\",\n",
    "    \"rmin_5D_mean\", \"rmax_5D_mean\", \"sph_5D_mean\",\n",
    "    \"vs_5D_mean\", \"th_5D_mean\", \"srad_5D_mean\", \"etr_5D_mean\",\n",
    "    \"fm100_5D_mean\", \"fm1000_5D_mean\", \"bi_5D_mean\", \"vpd_5D_mean\", \"erc_5D_mean\",\n",
    "    # topographie\n",
    "    \"Elevation\", \"Slope\", \"Aspect\", \"TRI\", \"TPI\",\n",
    "    # NDVI agrégé\n",
    "    \"NDVI_mean_scalar\",\n",
    "]\n",
    "\n",
    "# Variables catégorielles (encodage one-hot)\n",
    "features_categorical = [\n",
    "    \"EVT\", \"EVH\", \"EVC\",\n",
    "    \"Ecoregion_US_L3CODE\", \"Ecoregion_US_L4CODE\",\n",
    "]\n",
    "\n",
    "target = \"FIRE_SIZE\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Sous-dataframe + nettoyage basique de FIRE_SIZE\n",
    "# -------------------------------------------------\n",
    "df_model = df[features_numeric + features_categorical + [target]].copy()\n",
    "\n",
    "# On ne garde que les feux avec une taille strictement positive\n",
    "df_model = df_model[df_model[target] > 0]\n",
    "\n",
    "# Conversion des colonnes numériques en float (sécurisée)\n",
    "for col in features_numeric:\n",
    "    df_model[col] = pd.to_numeric(df_model[col], errors=\"coerce\")\n",
    "\n",
    "# Transformation de la cible en log pour réduire l'effet des mega-feux\n",
    "df_model[\"target_log\"] = np.log1p(df_model[target])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Split train / test\n",
    "# -----------------------------\n",
    "X = df_model[features_numeric + features_categorical]\n",
    "y = df_model[\"target_log\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6. Pipeline de prétraitement (Green & propre)\n",
    "#    - Imputation des NaN\n",
    "#    - Standardisation des numériques\n",
    "#    - One-hot des catégorielles\n",
    "# --------------------------------------------\n",
    "from sklearn.pipeline import Pipeline as SkPipeline  # pour éviter confusion\n",
    "\n",
    "numeric_pipeline = SkPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = SkPipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, features_numeric),\n",
    "        (\"cat\", categorical_pipeline, features_categorical),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Modèle baseline : ElasticNet\n",
    "#    -> modèle linéaire, simple, peu coûteux\n",
    "# -----------------------------\n",
    "model_enet = SkPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42))\n",
    "])\n",
    "\n",
    "model_enet.fit(X_train, y_train)\n",
    "y_pred_enet = model_enet.predict(X_test)\n",
    "\n",
    "print(\"=== ElasticNet (baseline Green AI) ===\")\n",
    "print(\"RMSE log :\", np.sqrt(mean_squared_error(y_test, y_pred_enet)))\n",
    "print(\"MAE log  :\", mean_absolute_error(y_test, y_pred_enet))\n",
    "print(\"R² log   :\", r2_score(y_test, y_pred_enet))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Modèle principal : XGBoost\n",
    "#    -> modèle non linéaire MAIS réglé de façon raisonnable\n",
    "#       (nombre d'arbres limité, tree_method='hist', pas de tuning lourd)\n",
    "# -----------------------------\n",
    "model_xgb = SkPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", XGBRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",   # version plus efficace / moins coûteuse\n",
    "        n_jobs=4,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost (log) ===\")\n",
    "print(\"RMSE log :\", np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
    "print(\"MAE log  :\", mean_absolute_error(y_test, y_pred_xgb))\n",
    "print(\"R² log   :\", r2_score(y_test, y_pred_xgb))\n",
    "\n",
    "# En unités réelles (acres)\n",
    "y_test_real = np.expm1(y_test)\n",
    "y_pred_real = np.expm1(y_pred_xgb)\n",
    "\n",
    "print(\"\\n=== XGBoost (surface réelle) ===\")\n",
    "print(\"RMSE réel :\", np.sqrt(mean_squared_error(y_test_real, y_pred_real)))\n",
    "print(\"MAE réel  :\", mean_absolute_error(y_test_real, y_pred_real))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 9. Interprétation rapide : features les plus importantes\n",
    "# ----------------------------------------------------\n",
    "# On récupère les noms de features après préprocessing\n",
    "feature_names = model_xgb.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "importances = model_xgb.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# top 15\n",
    "idx_sorted = np.argsort(importances)[::-1][:15]\n",
    "print(\"\\n=== Top 15 features (XGBoost) ===\")\n",
    "for name, imp in zip(feature_names[idx_sorted], importances[idx_sorted]):\n",
    "    print(f\"{name:40s} -> {imp:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
